Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Poulos2018,
abstract = {Missing data imputation can help improve the performance of prediction models in situations where missing data hide useful information. This paper compares methods for imputing missing categorical data for supervised classification tasks. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different levels of additional missing-data perturbation. We show imputation methods can increase predictive accuracy in the presence of missing-data perturbation, which can actually improve prediction accuracy by regularizing the classifier. We achieve results comparable to the state-of-the-art on the Adult dataset with missing-data perturbation and k-nearest-neighbors (k-NN) imputation.},
archivePrefix = {arXiv},
arxivId = {1610.09075},
author = {Poulos, Jason and Valle, Rafael},
doi = {10.1080/08839514.2018.1448143},
eprint = {1610.09075},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Imputation-For-Supervised-Learning.pdf:pdf},
issn = {10876545},
journal = {Applied Artificial Intelligence},
keywords = {at http,at https,com,decision trees,github,imputation methods,io,jvpoulos,mdi,mdi-supp,missing data,neural networks,papers,pdf,perturbation,rafaelvalle,random forests,supplementary material is available,the code used for,the online,this project is available,we thank},
number = {2},
pages = {186--196},
title = {{Missing Data Imputation for Supervised Learning}},
volume = {32},
year = {2018}
}
@article{Graham2009,
abstract = {This review presents a practical summary of the missing data literature, including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. Practical missing data analysis issues are discussed, most notably the inclusion of auxiliary variables for improving power and reducing bias. Solutions are given for missing data challenges such as handling longitudinal, categorical, and clustered data with normal-model MI; including interactions in the missing data model; and handling large numbers of variables. The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. Strategies suggested for reducing attrition bias include using auxiliary variables, collecting follow-up data on a sample of those initially missing, and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.},
author = {Graham, John W.},
doi = {10.1146/annurev.psych.58.110405.085530},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Analysis-Making-It-Work-In-The-Real-World.pdf:pdf},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {attrition,maximum likelihood,missingness,multiple imputation,nonignorable,planned missingness},
number = {1},
pages = {549--576},
title = {{Missing Data Analysis: Making It Work in the Real World}},
volume = {60},
year = {2009}
}
@article{Kelderman2019,
author = {Kelderman, Keene and Kunstman, Ben and Roy, Hayley and Sivakumar, Namratha and Mccormick, Samantha and Bernhardt, Courtney},
file = {:C$\backslash$:/Users/theto/Documents/harvard-summer-biostats/literature/National-Coal-Ash-Report-3.4.19-1.pdf:pdf},
title = {{Coal's Poisonous Legacy: Groundwater Contaminated by Coal Ash Across the U.S.}},
year = {2019}
}
@article{Meijs2018,
author = {Meijs, Amber Van Der},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Imputation-Predicting-Missing-Values.pdf:pdf},
number = {July},
title = {{Missing Data Imputation : Predicting Missing Values}},
year = {2018}
}
