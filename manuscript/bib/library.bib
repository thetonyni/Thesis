Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Hornung1989,
author = {Hornung, Richard and Reed, Laurence},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/estimation of average concentration in the presence of nd values.pdf:pdf},
journal = {Applied Occupational and Environmental Hygiene},
number = {1},
pages = {46--51},
title = {{Estimation of Average Concentration in the Presence of Nondetectable Values}},
volume = {5},
year = {1989}
}
@article{Reif2012,
abstract = {The performance of most of the classification algorithms on a particular dataset is highly dependent on the learning parameters used for training them. Different approaches like grid search or genetic algorithms are frequently employed to find suitable parameter values for a given dataset. Grid search has the advantage of finding more accurate solutions in general at the cost of higher computation time. Genetic algorithms, on the other hand, are able to find good solutions in less time, but the accuracy of these solutions is usually lower than those of grid search. This paper uses ideas from meta-learning and case-based reasoning to provide good starting points to the genetic algorithm. The presented approach reaches the accuracy of grid search at a significantly lower computational cost. We performed extensive experiments for optimizing learning parameters of the Support Vector Machine (SVM) and the Random Forest classifiers on over 100 datasets from UCI and StatLib repositories. For the SVM classifier, grid search achieved an average accuracy of 81 {\%} and took six hours for training, whereas the standard genetic algorithm obtained 74 {\%} accuracy in close to one hour of training. Our method was able to achieve an average accuracy of 81 {\%} in only about 45 minutes. Similar results were achieved for the Random Forest classifier. Besides a standard genetic algorithm, we also compared the presented method with three state-of-the-art optimization algorithms: Generating Set Search, Dividing Rectangles, and the Covariance Matrix Adaptation Evolution Strategy. Experimental results show that our method achieved the highest average accuracy for both classifiers. Our approach can be particularly useful when training classifiers on large datasets where grid search is not feasible. {\textcopyright} 2012 The Author(s).},
author = {Reif, Matthias and Shafait, Faisal and Dengel, Andreas},
doi = {10.1007/s10994-012-5286-7},
file = {:C$\backslash$:/Users/theto/Documents/git/STAT495F20-project-Ni/report/literature/meta-learning for evolutionary parameter optimization of classifiers.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Feature selection,Genetic algorithm,Meta-learning,Parameter optimization},
number = {3},
pages = {357--380},
title = {{Meta-learning for evolutionary parameter optimization of classifiers}},
volume = {87},
year = {2012}
}
@book{Bishop2006,
address = {Singapore},
author = {Bishop, Christopher},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:pdf},
isbn = {9780387310732},
publisher = {Springer Science and Business Media},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{May2012,
abstract = {Data subject to detection limits appear in a wide variety of studies. Data subject to detection limits are usually left-censored at the detection limit, often due to limitations in the measurement procedure being used. This thesis addresses three issues common to the analysis of data subject to detection limits. The first of these is the estimation of the limit of detection using repeated measurements from known analyte concentrations. An innovative change-point model is proposed to more accurately model the standard deviation of measured analyte concentrations, resulting in improved estimation of the limit of detection. The pro- posed methodology is applied to copy number data from an HIV pilot study. The second topic concerns estimation using generalized linear models when multiple covariates are subject to a limit of detection. We propose a Monte Carlo version of the EM algorithm similar to that in Ibrahim, Lipsitz, and Chen to handle a large number of covariates subject to detection limits in generalized linear models. Censored covariate values are sampled using the Adaptive Rejec- tion Metropolis Algorithm of Gilks, Best, and Tan. This procedure is applied to data from the National Health and Nutrition Examination Survey (NHANES), in which values of urinary heavy metals are subject to a limit of detection. Through simulation studies, we show that the proposed approach can lead to a significant reduction in variance for parameter estimates in these models, improving the power of such studies. The third and final topic addresses the joint modeling of longitudinal and survival data using time-varying covariates that are both intermittently missing and subject to a limit of detection. The model is motivated by data from the Multicenter AIDS Cohort Study (MACS), in which HIV+ subjects have viral load and CD4 cell counts measured at repeated visits along with survival data. The viral load data is subject to both left-censoring due to detection limits (17{\%}) and intermittent missing- ness (27{\%}). A Bayesian analysis is conducted on the MACS data using the proposed joint model. The proposed method is shown to improve the precision of estimates when compared to alternative methods.},
author = {May, Ryan C},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimation{\_}Methods{\_}for{\_}Data{\_}Subject{\_}to{\_}Detection{\_}Limits.pdf:pdf},
pages = {82},
title = {{Estimation Methods for Data Subject to Detection Limits}},
year = {2012}
}
@article{Slymen1994,
abstract = {Values below a specified detection limit are a common occurrence in environmental measurement which complicates statistical analysis. This paper addresses the problem of how these data may be included in hypothesis testing and demonstrates a solution that uses a regression model in the SAS statistical package with the ability to accommodate left-censored data. To illustrate, the LIFEREG procedure is applied to data for an analyte (tin) found in fish tissue during a multifactorial bioaccumulation study involving two experimental sites, three experiments at each site, and sampling on multiple days. The results of this analysis are presented, indicating evidence of bioaccumulation but no significant differences between sites or experiments. While regression methods to accommodate left-censored data are not new, their use in environmental studies does not appear to be widely practiced. The strengths and limitations of this approach are discussed. {\textcopyright} 1994, American Chemical Society. All rights reserved.},
author = {Slymen, Donald J. and {De Peyster}, Ann and Donohoe, Regina R.},
doi = {10.1021/es00054a022},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Hypothesis Testing with Values below Detection Limit in Environmental Studies.pdf:pdf},
issn = {15205851},
journal = {Environmental Science and Technology},
number = {5},
pages = {898--902},
title = {{Hypothesis Testing with Values below Detection Limit in Environmental Studies}},
volume = {28},
year = {1994}
}
@article{Morris2019,
abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
archivePrefix = {arXiv},
arxivId = {1712.03198},
author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
doi = {10.1002/sim.8086},
eprint = {1712.03198},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/article{\_}Morris-et-al{\_}2019{\_}{\_}Using-simulation-studies-to-evaluate-statistical-methods.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {Monte Carlo,graphics for simulation,simulation design,simulation reporting,simulation studies},
number = {11},
pages = {2074--2102},
pmid = {30652356},
title = {{Using simulation studies to evaluate statistical methods}},
volume = {38},
year = {2019}
}
@article{Thompson2003,
abstract = {Laboratory analyses in a variety of contexts may result in left- and interval-censored measurements. We develop and evaluate a maximum likelihood approach to linear regression analysis in this setting and compare this approach to commonly used simple substitution methods. We explore via simulation the impact on bias and power of censoring fraction and sample size in a range of settings. The maximum likelihood approach represents only a moderate increase in power, but we show that the bias in substitution estimates may be substantial.},
author = {Thompson, Mary Lou and Nelson, Kerrie P.},
doi = {10.1023/A:1023630425376},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/lienar regression with type I interval and left-censored response data.pdf:pdf},
issn = {13528505},
journal = {Environmental and Ecological Statistics},
keywords = {Environmental data,Laboratory analysis,Maximum likelihood},
number = {2},
pages = {221--230},
title = {{Linear regression with Type I interval-and left-censored response data}},
volume = {10},
year = {2003}
}
@article{Walther2005,
author = {Walther, Bruno A and Moore, Joslin L},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/The concept of bias precision and accuracy.pdf:pdf},
number = {July},
title = {{The concepts of bias , precision and accuracy , and their use in testing the performance of species richness estimators , with a literature review of estimator performance}},
volume = {6},
year = {2005}
}
@techreport{EPA2009,
author = {{Environmental Protection Agency}},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Statistical Analysis of Groundwater Monitoring Data At RCRA Facilities.pdf:pdf},
title = {{STATISTICAL ANALYSIS OF GROUNDWATER MONITORING DATA AT RCRA FACILITIES UNIFIED GUIDANCE}},
year = {2009}
}
@article{Cohen-addad2019,
author = {Cohen-addad, Vincent and Universit{\'{e}}, Sorbonne},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/ContentServer.pdf:pdf},
number = {4},
title = {{Hierarchical Clustering : Objective Functions and Algorithms}},
volume = {66},
year = {2019}
}
@misc{Pohlmann92,
author = {Pohlmann, K. F.},
file = {:C$\backslash$:/Users/theto/Downloads/groundwaterissue{\_}errors{\_}sampling.pdf:pdf},
publisher = {EPA},
title = {{Potential Sources of Error in Groundwater Sampling at Hazardous Waste Sites}},
year = {1992}
}
@article{He2020,
abstract = {Measures of substance concentration in urine, serum or other biological matrices often have an assay limit of detection. When concentration levels fall below the limit, the exact measures cannot be obtained. Instead, the measures are censored as only partial information that the levels are under the limit is known. Assuming the concentration levels are from a single population with a normal distribution or follow a normal distribution after some transformation, Tobit regression models, or censored normal regression models, are the standard approach for analyzing such data. However, in practice, it is often the case that the data can exhibit more censored observations than what would be expected under the Tobit regression models. One common cause is the heterogeneity of the study population, caused by the existence of a latent group of subjects who lack the substance measured. For such subjects, the measurements will always be under the limit. If a censored normal regression model is appropriate for modeling the subjects with the substance, the whole population follows a mixture of a censored normal regression model and a degenerate distribution of the latent class. While there are some studies on such mixture models, a fundamental question about testing whether such mixture modeling is necessary, i.e. whether such a latent class exists, has not been studied yet. In this paper, three tests including Wald test, likelihood ratio test and score test are developed for testing the existence of such latent class. Simulation studies are conducted to evaluate the performance of the tests, and two real data examples are employed to illustrate the tests.},
author = {He, Hua and Tang, Wan and Kelly, Tanika and Li, Shengxu and He, Jiang},
doi = {10.1177/0962280219885985},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Statistical{\_}Tests{\_}For{\_}Latent{\_}Class{\_}In{\_}Censored{\_}Data{\_}Due{\_}To{\_}Detection{\_}Limit.pdf:pdf},
issn = {14770334},
journal = {Statistical Methods in Medical Research},
keywords = {Censored normal regression,Tobit model,Wald test,detection limit,latent class,likelihood ratio test,mixture Tobit model,score test},
number = {8},
pages = {2179--2197},
pmid = {31736411},
title = {{Statistical tests for latent class in censored data due to detection limit}},
volume = {29},
year = {2020}
}
@article{Lafleur2011,
abstract = {In analytic chemistry a detection limit (DL) is the lowest measurable amount of an analyte that can be distinguished from a blank; many biomedical measurement technologies exhibit this property. From a statistical perspective, these data present inferential challenges because instead of precise measures, one only has information that the value is somewhere between 0 and the DL (below detection limit, BDL). Substitution of BDL values, with 0 or the DL can lead to biased parameter estimates and a loss of statistical power. Statistical methods that make adjustments when dealing with these types of data, often called left-censored data, are available in many commercial statistical packages. Despite this availability, the use of these methods is still not widespread in biomedical literature. We have reviewed the statistical approaches of dealing with BDL values, and used simulations to examine the performance of the commonly used substitution methods and the most widely available statistical methods. We have illustrated these methods using a study undertaken at the Vanderbilt-Ingram Cancer Center, to examine the serum bile acid levels in patients with colorectal cancer and adenoma. We have found that the modern methods for BDL values identify disease-related differences that are often missed, with statistically naive approaches.},
author = {Lafleur, Bonnie and Lee, Wooin and Billhiemer, Dean and Lockhart, Craig and Liu, Junmei and Merchant, Nipun},
doi = {10.4103/1477-3163.79681},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Statistical methods for assays with limits of detection{\_} Serum bile acid as a differentiator between patients with normal colons, adenomas, and colorectal cancer.pdf:pdf},
issn = {09746773},
journal = {Journal of Carcinogenesis},
keywords = {Bile acids,colorectal cancer,detection limits,statistical methods},
pages = {1--8},
pmid = {21712958},
title = {{Statistical methods for assays with limits of detection: Serum bile acid as a differentiator between patients with normal colons, adenomas, and colorectal cancer}},
volume = {10},
year = {2011}
}
@article{Yang2020,
abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.},
archivePrefix = {arXiv},
arxivId = {2007.15745},
author = {Yang, Li and Shami, Abdallah},
doi = {10.1016/j.neucom.2020.07.061},
eprint = {2007.15745},
file = {:C$\backslash$:/Users/theto/Documents/git/STAT495F20-project-Ni/report/literature/on{\_}hyperparameter{\_}optimization{\_}of{\_}ml{\_}algorithms{\_}theory{\_}and{\_}practice.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Bayesian optimization,Genetic algorithm,Grid search,Hyper-parameter optimization,Machine learning,Particle swarm optimization},
pages = {295--316},
publisher = {Elsevier B.V.},
title = {{On hyperparameter optimization of machine learning algorithms: Theory and practice}},
url = {https://doi.org/10.1016/j.neucom.2020.07.061},
volume = {415},
year = {2020}
}
@book{Hutter2014,
author = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-319-00960-5_6},
file = {:C$\backslash$:/Users/theto/Documents/git/STAT495F20-project-Ni/report/literature/automated machine learning.pdf:pdf},
isbn = {9783319009599},
issn = {1860949X},
pages = {35--62},
title = {{Automated Machine Learning: Methods, Systems, Challenges}},
volume = {498},
year = {2014}
}
@article{Hall2020,
abstract = {This study compared four different statistical methods, involving six estimation procedures, for addressing censored left data in measuring temporal trends of eight different pyrethroids measured in sediment from a 10-year data set in a residential California stream (Pleasant Grove Creek). The statistical methods used were: the Kaplan–Meier (km) method; the robust regression on order statistics (ros using normal and log normal distributions rosln); the maximum likelihood estimation (mlen using normal and log normal distributions mleln); and a substitution method (sub) using ½ the detection limit. For five of the eight pyrethroids (bifenthrin, cyfluthrin, cypermethrin, lambda-cyhalothrin, and permethrin), the six statistical methods generally agree, with one exception, that the data set exhibit significant declining trends. In the case of bifenthrin, the slight disagreement among statistical methods only occurred for the mleln estimate that did not show a significant declining trend, whereas the other five methods did. For deltamethrin, esfenvalerate, and fenpropathrin, all six statistical methods were in agreement showing no significant trends. Possible reasons for declining sediment concentrations of pyrethroids in Pleasant Grove Creek are urban label changes effective in 2012–2015 that reduced residential use, variable annual rainfall, and more responsible homeowner use based on outreach/education programs.},
author = {Hall, Lenwood W. and Perry, Elgin and Anderson, Ronald D.},
doi = {10.1007/s00244-020-00769-0},
file = {:C$\backslash$:/Users/theto/Downloads/Hall2020{\_}Article{\_}AComparisonOfDifferentStatisti.pdf:pdf},
issn = {14320703},
journal = {Archives of Environmental Contamination and Toxicology},
number = {4},
pages = {508--523},
pmid = {33074408},
publisher = {Springer US},
title = {{A Comparison of Different Statistical Methods for Addressing Censored Left Data in Temporal Trends Analysis of Pyrethroids in a California Stream}},
url = {https://doi.org/10.1007/s00244-020-00769-0},
volume = {79},
year = {2020}
}
@article{Antweiler2015,
author = {Antweiler, Ronald C},
doi = {10.1021/acs.est.5b02385},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/evaluation of statistical treatments.pdf:pdf},
title = {{Evaluation of Statistical Treatments of Left-Censored Environmental Data Using Coincident Uncensored Data Sets. II. Group Comparisons}},
year = {2015}
}
@article{Pracheil2016,
author = {Pracheil, Brenda M and Adams, S Marshall and Bevelhimer, Mark S and Fortner, Allison M and Greeley, Mark S and Cheryl, Jr and Teresa, A Murphy and Peterson, Mark J},
doi = {10.1007/s10646-016-1668-0},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Relating Fish Health - Coal Ash Spill.pdf:pdf},
pages = {1136--1149},
title = {{Relating fish health and reproductive metrics to contaminant bioaccumulation at the Tennessee Valley Authority Kingston coal ash spill site}},
year = {2016}
}
@article{Kelderman2019,
author = {Kelderman, Keene and Kunstman, Ben and Roy, Hayley and Sivakumar, Namratha and Mccormick, Samantha and Bernhardt, Courtney},
file = {:C$\backslash$:/Users/theto/Documents/harvard-summer-biostats/literature/National-Coal-Ash-Report-3.4.19-1.pdf:pdf},
title = {{Coal's Poisonous Legacy: Groundwater Contaminated by Coal Ash Across the U.S.}},
year = {2019}
}
@article{Berthouex1993,
author = {Berthouex, Paul},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A{\_}Study{\_}Of{\_}The{\_}Precision{\_}Of{\_}Lead{\_}...{\_}Near{\_}The{\_}Method{\_}LOD.pdf:pdf},
keywords = {are expected either to,be absent or to,bias,focus on chemicals that,in environmental quality,lead,limit of detection,many important scientific problems,monitoring,precision},
number = {5},
pages = {620--629},
title = {{A Study of the Precision of Lead Measurements at Concentrations Near the Method Limit of Detection}},
volume = {65},
year = {1993}
}
@article{Glass2001,
abstract = {A retrospective assessment of exposure to benzene was carried out for a nested case control study of lympho-haematopoietic cancers, including leukaemia, in the Australian petroleum industry. Each job or task in the industry was assigned a Base Estimate (BE) of exposure derived from task-based personal exposure assessments carried out by the company occupational hygienists. The BEs corresponded to the estimated arithmetic mean exposure to benzene for each job or task and were used in a deterministic algorithm to estimate the exposure of subjects in the study. Nearly all of the data sets underlying the BEs were found to contain some values below the limit of detection (LOD) of the sampling and analytical methods and some were very heavily censored; up to 95{\%} of the data were below the LOD in some data sets. It was necessary, therefore, to use a method of calculating the arithmetic mean exposures that took into account the censored data. Three different methods were employed in an attempt to select the most appropriate method for the particular data in the study. A common method is to replace the missing (censored) values with half the detection limit. This method has been recommended for data sets where much of the data are below the limit of detection or where the data are highly skewed; with a geometric standard deviation of 3 or more. Another method, involving replacing the censored data with the limit of detection divided by the square root of 2, has been recommended when relatively few data are below the detection limit or where data are not highly skewed. A third method that was examined is Cohen's method. This involves mathematical extrapolation of the left-hand tail of the distribution, based on the distribution of the uncensored data, and calculation of the maximum likelihood estimate of the arithmetic mean. When these three methods were applied to the data in this study it was found that the first two simple methods give similar results in most cases. Cohen's method on the other hand, gave results that were generally, but not always, higher than simpler methods and in some cases gave extremely high and even implausible estimates of the mean. It appears that if the data deviate substantially from a simple log-normal distribution, particularly if high outliers are present, then Cohen's method produces erratic and unreliable estimates. After examining these results, and both the distributions and proportions of censored data, it was decided that the half limit of detection method was most suitable in this particular study. {\textcopyright} 2001 British Occupational Hygiene Society.},
author = {Glass, D. C. and Gray, C. N.},
doi = {10.1016/S0003-4878(01)00022-9},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimating{\_}Mean{\_}Exposures{\_}From{\_}Censored{\_}Data.pdf:pdf},
issn = {00034878},
journal = {Annals of Occupational Hygiene},
keywords = {Benzene,Censored data,Epidemiology,Limit of detection,Retrospective exposure assessment},
number = {4},
pages = {275--282},
pmid = {11378148},
title = {{Estimating mean exposures from censored data: Exposure to benzene in the Australian petroleum industry}},
volume = {45},
year = {2001}
}
@article{Barnard1999,
abstract = {Rubin's multiple imputation is a three-step method for handling complex missing data, or more generally, incomplete-data problems, which arise frequently in medical studies. At the first step, m({\textgreater} 1) completed-data sets are created by imputing the unobserved data m times using m independent draws from an imputation model, which is constructed to reasonably approximate the true distributional relationship between the unobserved data and the available information, and thus reduce potentially very serious nonresponse bias due to systematic difference between the observed data and the unobserved ones. At the second step, m complete-data analyses are performed by treating each completed-data set as a real complete-data set, and thus standard complete-data procedures and software can be utilized directly. At the third step, the results from the m complete-data analyses are combined in a simple, appropriate way to obtain the so-called repeated-imputation inference, which properly takes into account the uncertainty in the imputed values. This paper reviews three applications of Rubin's method that are directly relevant for medical studies. The first is about estimating the reporting delay in acquired immune deficiency syndrome (AIDS) surveillance systems for the purpose of estimating survival time after AIDS diagnosis. The second focuses on the issue of missing data and noncompliance in randomized experiments, where a school choice experiment is used as an illustration. The third looks at handling nonresponse in United States National Health and Nutrition Examination Surveys (NHANES). The emphasis of our review is on the building of imputation models (i.e. the first step), which is the most fundamental aspect of the method.},
author = {Barnard, John and Meng, Xiao Li},
doi = {10.1191/096228099666230705},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/applications of multiple imputation studies in medical studies from AIDS to NHANES.pdf:pdf},
issn = {09622802},
journal = {Statistical Methods in Medical Research},
number = {1},
pages = {17--36},
pmid = {10347858},
title = {{Applications of multiple imputation in medical studies: From AIDS to NHANES}},
volume = {8},
year = {1999}
}
@article{Poulos2018,
abstract = {Missing data imputation can help improve the performance of prediction models in situations where missing data hide useful information. This paper compares methods for imputing missing categorical data for supervised classification tasks. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different levels of additional missing-data perturbation. We show imputation methods can increase predictive accuracy in the presence of missing-data perturbation, which can actually improve prediction accuracy by regularizing the classifier. We achieve results comparable to the state-of-the-art on the Adult dataset with missing-data perturbation and k-nearest-neighbors (k-NN) imputation.},
archivePrefix = {arXiv},
arxivId = {1610.09075},
author = {Poulos, Jason and Valle, Rafael},
doi = {10.1080/08839514.2018.1448143},
eprint = {1610.09075},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Imputation-For-Supervised-Learning.pdf:pdf},
issn = {10876545},
journal = {Applied Artificial Intelligence},
keywords = {at http,at https,com,decision trees,github,imputation methods,io,jvpoulos,mdi,mdi-supp,missing data,neural networks,papers,pdf,perturbation,rafaelvalle,random forests,supplementary material is available,the code used for,the online,this project is available,we thank},
number = {2},
pages = {186--196},
title = {{Missing Data Imputation for Supervised Learning}},
volume = {32},
year = {2018}
}
@article{Ganser2010,
author = {Ganser, Gary H and Hewett, Paul},
doi = {10.1080/15459621003609713},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/An Accurate Substitution Method for Analyzing Censored Data.pdf:pdf},
number = {April},
pages = {233--244},
title = {{An Accurate Substitution Method for Analyzing Censored Data}},
year = {2010}
}
@article{Pohlmann2002,
abstract = {The author reviewed the use and interpretation of factor analysis in articles published in The Journal of Educational Research articles from 1992 to 2002. He found all major forms of factor analysis among the 25 articles that he reviewed. Exploratory factor analysis was the most common application that he found. He noted only 3 applications of confirmatory factor analysis. In general, the author found that the analyses were performed appropriately and the results were presented informatively. In some studies, however, he found that the authors did not provide sufficient information about their analyses for readers to evaluate the results. The author provides a brief introduction to factor analysis history and models, along with an illustration of an exploratory factor analysis. He offers recommendations for the appropriate reporting and interpretation of a factor analysis.},
author = {Pohlmann, John T},
file = {:C$\backslash$:/Users/theto/Downloads/use of factor analysis.pdf:pdf},
keywords = {confirmatory factor analysis,exploratory factor analysis,factor analysis},
pages = {1992--2002},
title = {{Use and Interpretation of Factor Analysis in The Journal of Education Research: 1992-2002}},
year = {2002}
}
@article{Crovelli1993,
author = {Crovelli, Robert A},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/censored geochemical data.pdf:pdf},
keywords = {environmental science,lognormal distribution,qualified data,substitution method},
number = {1},
pages = {59--80},
title = {{An Objective Replacement Method for Censored Geochemical Data}},
volume = {25},
year = {1993}
}
@article{Chen2011,
author = {Chen, Haiying and Quandt, Sara A and Grzywacz, Joseph G and Arcury, Thomas A and Environmental, Source and Perspectives, Health and March, No and Chen, Haiying and Quandi, Sara A and Grzywacz, Joseph G and Arcury, Thomas A},
doi = {10.1289/ehp.l002124},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A{\_}Distribution{\_}Based{\_}Multiple{\_}Imputation{\_}Method{\_}For{\_}Handling{\_}...{\_}Values{\_}Below{\_}The{\_}LOD.pdf:pdf},
number = {3},
pages = {351--356},
title = {{A Distribution-Based Multiple Imputation Method for Handling Bivariate Pesticide Data with Values below the Limit of Detection}},
volume = {119},
year = {2011}
}
@article{Eloul2015,
abstract = {We derive approximate expressions for the average number of diffusive impacts/hits of nanoparticles on microdisc and microwire electrodes for the case where the impact leads to the loss of the nanoparticles from solution either via irreversible adsorption or complete electro-dissolution. The theory can also be applied to sub-micrometre size electrodes (nano-electrodes). The resulting equations can be utilised to analyse the number of impacts and its variance in the 'nano-impact' experiment. We also provide analytical expressions for the first passage time of an impact for dilute nanoparticle solutions in the continuum limit of Fickian diffusion. The expressions for the first passage times are used to estimate the lower limit of detection in ultra-dilute nanoparticle solutions for typical nano-impact experiments, and show the advantage of using microwire electrodes in ultra-dilute solutions or solutions containing larger nano-particles.},
author = {Eloul, Shaltiel and K{\"{a}}telh{\"{o}}n, Enno and Batchelor-McAuley, Christopher and Tschulik, Kristina and Compton, Richard G.},
doi = {10.1016/j.jelechem.2015.07.042},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Diffusional{\_}Impacts{\_}Of{\_}Nanaparticles...{\_}The{\_}Limit{\_}Of{\_}Detection{\_}and{\_}First{\_}Passage{\_}Statistics.pdf:pdf},
issn = {15726657},
journal = {Journal of Electroanalytical Chemistry},
keywords = {Microcylinder,Microdisc,Microwire electrode,Nano-impacts,Nanoparticle detection,Nanoparticle voltammetry,Ultra-low concentration,Wire electrode},
pages = {136--142},
publisher = {Elsevier B.V.},
title = {{Diffusional impacts of nanoparticles on microdisc and microwire electrodes: The limit of detection and first passage statistics}},
url = {http://dx.doi.org/10.1016/j.jelechem.2015.07.042},
volume = {755},
year = {2015}
}
@book{Clark2013,
abstract = {The purpose of this document is to provide a conceptual introduction to statistical or machine learning (ML) techniques for those that might not normally be exposed to such approaches during their required typical statistical training.},
author = {Clark, Michael},
booktitle = {An Introduction to Machine Learning with Applications in R},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/ML{\_}inR.pdf:pdf},
keywords = {data minin,machine learning,statistical learning},
pages = {1--43},
title = {{An Introduction to Machine Learning with Applications in R}},
year = {2013}
}
@book{Lantz2013,
address = {Birmingham},
author = {Lantz, Brett},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/MachineLearningR{\_}{\_}Brett{\_}Lantz.pdf:pdf},
isbn = {9781782162148},
publisher = {Packt Publishing},
title = {{Machine Learning with R}},
year = {2013}
}
@article{Gilliom1986,
abstract = {A recurring difficulty encountered in investigations of many metals and organic contaminants in ambient waters is that a substantial portion of water sample concentrations are below limits of detection established by analytical laboratories. Several methods were evaluated for estimating distributional parameters for such censored data sets using only uncensored observations. Their reliabilities were evaluated by a Monte Carlo experiment in which small samples were generated from a wide range of parent distributions and censored at varying levels. Eight methods were used to estimate the mean, standard deviation, median, and interquartile range. Criteria were developed, based on the distribution of uncensored observations, for determining the best performing parameter estimation method for any particular data set. The most robust method for minimizing error in censored‐sample estimates of the four distributional parameters over all simulation conditions was the log‐probability regression method. With this method, censored observations are assumed to follow the zero‐to‐censoring level portion of a lognormal distribution obtained by a least squares regression between logarithms of uncensored concentration observations and their z scores. When method performance was separately evaluated for each distributional parameter over all simulation conditions, the log‐probability regression method still had the smallest errors for the mean and standard deviation, but the lognormal maximum likelihood method had the smallest errors for the median and interquartile range. When data sets were classified prior to parameter estimation into groups reflecting their probable parent distributions, the ranking of estimation methods was similar, but the accuracy of error estimates was markedly improved over those without classification. This paper is not subject to U.S. copyright. Published in 1986 by the American Geophysical Union.},
author = {Gilliom, Robert J. and Helsel, Dennis R.},
doi = {10.1029/WR022i002p00135},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimation of Distributional Parameters for Censored Trace Level Water Quality Data Estimation Techniques.pdf:pdf},
issn = {19447973},
journal = {Water Resources Research},
number = {2},
pages = {135--146},
title = {{Estimation of Distributional Parameters for Censored Trace Level Water Quality}},
volume = {22},
year = {1986}
}
@article{Shoari2018,
author = {Shoari, Niloofar},
doi = {10.1002/etc.4046},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/toward improved analysis of concentration data embracing nondetects.pdf:pdf},
keywords = {environmental data analysis,exposure assessment,left-censored,limit of detection,nondetect},
number = {3},
pages = {643--656},
title = {{Toward Improved Analysis of Concentration Data : Embracing Nondetects}},
volume = {37},
year = {2018}
}
@article{Antweiler2008,
abstract = {The main classes of statistical treatment of below-detection limit (left-censored) environmental data for the determination of basic statistics that have been used in the literature are substitution methods, maximum likelihood, regression on order statistics (ROS), and nonparametric techniques. These treatments, along with using all instrument-generated data (even those below detection), were evaluated by examining data sets in which the true values of the censored data were known. It was found that for data sets with less than 70{\%} censored data, the best technique overall for determination of summary statistics was the nonparametric Kaplan-Meier technique. ROS and the two substitution methods of assigning one-half the detection limit value to censored data or assigning a random number between zero and the detection limit to censored data were adequate alternatives. The use of these two substitution methods, however, requires a thorough understanding of how the laboratory censored the data. The technique of employing all instrument-generated data - including numbers below the detection limit - was found to be less adequate than the above techniques. At high degrees of censoring (greater than 70{\%} censored data), no technique provided good estimates of summary statistics. Maximum likelihood techniques were found to be far inferior to all other treatments except substituting zero or the detection limit value to censored data.},
author = {Antweiler, Ronald C. and Taylor, Howard E.},
doi = {10.1021/es071301c},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Evaluation-Of-Statistical-Treatments-Of-Left-Censored-Environmental-Data.pdf:pdf},
issn = {0013936X},
journal = {Environmental Science and Technology},
number = {10},
pages = {3732--3738},
pmid = {18546715},
title = {{Evaluation of statistical treatments of left-censored environmental data using coincident uncensored data sets: I. Summary statistics}},
volume = {42},
year = {2008}
}
@article{Canales2018,
author = {Canales, Robert},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Methods-For-Handling-Left-Censored-Data-in-Quantitative.pdf:pdf},
keywords = {accepted 8 august,left censored,limit of detection,quantitative microbial risk assessment,received 18 may 2018},
number = {20},
pages = {1--10},
title = {{Methods for Handling Left-Censored Data in Quantitative Microbial Risk Assessment}},
volume = {84},
year = {2018}
}
@article{Bolks2014,
abstract = {Through the National Nonpoint Source Monitoring Program (NNPSMP), states monitor and evaluate a subset of watershed projects funded by the Clean Water Act Section 319 Nonpoint Source Control Program. The program has two major objectives: 1. To scientifically evaluate the effectiveness of watershed technologies designed to control nonpoint source pollution 2. To improve our understanding of nonpoint source pollution NNPSMP Tech Notes is a series of publications that shares this unique research and monitoring effort. It offers guidance on data collection, implementation of pollution control technologies, and monitoring design, as well as case studies that illustrate principles in action.},
author = {Bolks, A and DeWire, A and Harcum, J.B.},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/tech{\_}notes{\_}10{\_}jun2014{\_}r.pdf:pdf},
journal = {Technotes},
number = {2},
pages = {153--172},
title = {{Baseline Assessment of Left-Censored Environmental Data Using R}},
url = {https://www.epa.gov/sites/production/files/2016-05/documents/tech{\_}notes{\_}10{\_}jun2014{\_}r.pdf},
volume = {9},
year = {2014}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. {\textcopyright} 2012 James Bergstra and Yoshua Bengio.},
author = {Bergstra, James and Bengio, Yoshua},
file = {:C$\backslash$:/Users/theto/Documents/git/STAT495F20-project-Ni/report/literature/random search for hyperparamter optimization.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Global optimization,Model selection,Neural networks,Response surface modeling},
pages = {281--305},
title = {{Random search for hyper-parameter optimization}},
volume = {13},
year = {2012}
}
@article{Ruhl2010,
author = {Ruhl, Laura and Vengosh, Avner and Dwyer, Gary and Hsu-Kim, Heileen and Deonarine, Amrika},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Environmental Impacts of the Coal Ash Spill in Kingston Tennessee An 18 Month Study.pdf:pdf},
number = {24},
pages = {9272--9278},
title = {{Environmental Impacts of the Coal Ash Spill in Kingston , Tennessee: An 18-Month Survey}},
volume = {44},
year = {2010}
}
@article{Vanschoren2018,
abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
archivePrefix = {arXiv},
arxivId = {1810.03548},
author = {Vanschoren, Joaquin},
eprint = {1810.03548},
file = {:C$\backslash$:/Users/theto/Documents/git/STAT495F20-project-Ni/report/literature/meta-learning{\_}a survey.pdf:pdf},
journal = {arXiv},
pages = {1--29},
title = {{Meta-Learning: A Survey}},
year = {2018}
}
@article{Schmoyer1996,
abstract = {A frequent assumption in environmental risk assessment is that the underlying distribution of an analyte concentration is lognormal. However, the distribution of a random variable whose log has a t-distribution has infinite mean. Because of the proximity of the standard normal and t-distribution, this suggests that a distribution such as the gamma or truncated normal, with smaller right tail probabilities, might make a better statistical model for mean estimation than the lognormal. In order to assess the effect of departures from lognormality on lognormal-based statistics, we simulated complete lognormal, truncated normal, and gamma data for various sample sizes and coefficients of variation. In these cases, departures from lognormality were not easily detected with the Shapiro-Wilk test. Various lognormal-based estimates and tests were compared with alternate methods based on the ordinary sample mean and standard error. The examples were also considered in the presence of random left censoring with the mean and standard error of the product limit estimate replacing the ordinary sample mean and standard error. The results suggest that in the estimation of or tests about a mean, if the assumption of lognormality is at all suspect, then lognormal-based approaches may not be as good as the alternative methods. {\textcopyright} 1996 Chapman {\&} Hall.},
author = {Schmoyer, R. L. and Beauchamp, J. J. and Brandt, C. C. and Hoffman, F. O.},
doi = {10.1007/bf00577325},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/difficulties with the lognormal mean in mean estimation and testing.pdf:pdf},
issn = {13528505},
journal = {Environmental and Ecological Statistics},
keywords = {Gamma distribution,Left censoring,Product limit estimate,Risk assessment,Truncated normal distribution},
number = {1},
pages = {81--97},
title = {{Difficulties with the lognormal model in mean estimation and testing}},
volume = {3},
year = {1996}
}
@article{Graham2009,
abstract = {This review presents a practical summary of the missing data literature, including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. Practical missing data analysis issues are discussed, most notably the inclusion of auxiliary variables for improving power and reducing bias. Solutions are given for missing data challenges such as handling longitudinal, categorical, and clustered data with normal-model MI; including interactions in the missing data model; and handling large numbers of variables. The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. Strategies suggested for reducing attrition bias include using auxiliary variables, collecting follow-up data on a sample of those initially missing, and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.},
author = {Graham, John W.},
doi = {10.1146/annurev.psych.58.110405.085530},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Analysis-Making-It-Work-In-The-Real-World.pdf:pdf},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {attrition,maximum likelihood,missingness,multiple imputation,nonignorable,planned missingness},
number = {1},
pages = {549--576},
title = {{Missing Data Analysis: Making It Work in the Real World}},
volume = {60},
year = {2009}
}
@article{Ohio2007,
author = {{Environmental Protection Agency}},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/TGM-5.pdf:pdf},
title = {{Monitoring Well Placement}},
year = {2007}
}
@article{Byer2019,
author = {Byer, David and Carlson, Kenneth H},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Real{\_}Time{\_}Detection{\_}Of{\_}Intentional{\_}Chemical{\_}Contamination{\_}In{\_}The{\_}Distribution{\_}System.pdf:pdf},
number = {7},
pages = {130--133},
title = {{Real-time detection of intentional chemical contamination in the distributional system}},
volume = {97},
year = {2019}
}
@misc{Qualicum2007,
address = {British Columbia},
author = {{Parksville Qualicum News}},
file = {:C$\backslash$:/Users/theto/Downloads/Groundwater{\_}what{\_}is{\_}it{\_}{\_}Parksville{\_}Qualicum{\_}News{\_}British{\_}Columbia{\_}Canada{\_}{\_}{\_}November{\_}23{\_}2007{\_}{\_}p0a23.pdf:pdf},
month = {nov},
title = {{Groundwater: What Is It?}},
year = {2007}
}
@article{Probst2019,
abstract = {The random forest (RF) algorithm has several hyperparameters that have to be set by the user, for example, the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain, and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures. It is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a presenting brief overview of tuning strategies, we demonstrate the application of one of the most established tuning strategies, model-based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters. This article is categorized under: Algorithmic Development {\textgreater} Biological Data Mining Algorithmic Development {\textgreater} Statistics Algorithmic Development {\textgreater} Hierarchies and Trees Technologies {\textgreater} Machine Learning.},
archivePrefix = {arXiv},
arxivId = {1804.03515},
author = {Probst, Philipp and Wright, Marvin N. and Boulesteix, Anne Laure},
doi = {10.1002/widm.1301},
eprint = {1804.03515},
file = {:C$\backslash$:/Users/theto/Documents/git/STAT495F20-project-Ni/report/literature/hyperparameter and tuning strategies for random forest.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
keywords = {ensemble,literature review,out-of-bag,performance evaluation,ranger,sequential model-based optimization,tuning parameter},
number = {3},
pages = {1--15},
title = {{Hyperparameters and tuning strategies for random forest}},
volume = {9},
year = {2019}
}
@book{Gilbert1987,
address = {New York},
author = {Gilbert, Richard O.},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Statistical Methods for Environmental Pollution Monitoring.pdf:pdf},
isbn = {0442230508},
publisher = {Van Nostrand Reinhold Co.},
title = {{Statistical Methods for Environmental Pollution Monitoring}},
year = {1987}
}
@article{Akard2002,
abstract = {With the need for emission measurements of super ultra low emission vehicles (SULEV), analyzer manufacturers have been required to produce more precise and accurate analyzers. In order to compare analyzers, the customer must understand the different specifications used by the analyzer manufacturers. One specification that some manufacturers have used is the limit of detection (LOD) to indicate the reliability of the analyzer output at low concentrations. There are various methods for determining the LOD for a given analyzer. The authors will demonstrate how variations in methodology can produce different LOD values for a specific analyzer and what it means for the automotive emission analyzers. It is also demonstrated that the standard deviations of a zero signal, which is related to LOD, can be heavily influenced by data processing, such as data length in use and/or data smoothing. The LOD values obtained will be compared to the limit of quantification (LOQ) for that analyzer. In order to evaluate the utility of LOD measurements, various analyzer performances will be modeled and evaluated. While the LOD is a valid statistical approach to determining the presence of a component in a sample matrix, it is not directly applicable to emissions measurements. A discussion about the emission measurement requirements will demonstrate the limited applicability of this analyzer specification. Alternative methods for comparing analyzer specifications will be presented and justified. Copyright {\textcopyright} 2002 Society of Automotive Engineers, Inc.},
author = {Akard, Michael and Tsurumi, Kazuya and Oestergaard, Karl and Inoue, Kaori},
doi = {10.4271/2002-01-2711},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Why{\_}The{\_}LOD{\_}Is{\_}Not{\_}Appropriate{\_}Spec{\_}for{\_}Automotive{\_}Emissions.pdf:pdf},
issn = {26883627},
journal = {SAE Technical Papers},
number = {2002},
pages = {1321--1328},
title = {{Why the Limit of Detection (LOD) Value is Not an Appropriate Specification for Automotive Emissions Analyzers}},
volume = {111},
year = {2002}
}
@article{Perkowski1996,
author = {Perkowski, Ernest},
file = {:C$\backslash$:/Users/theto/Downloads/Impact of Ensemble Machine Learning Methods on Handling Missing Data.pdf:pdf},
keywords = {adaboost,bagging,boosting,data cleaning,data cleansing,ensemble,learning,machine,missing data,ml},
pages = {1--8},
title = {{Impact of Ensemble Machine Learning Methods on Handling Missing Data}},
year = {1996}
}
@article{Kroll1996,
abstract = {Censored data sets are often encountered in water quality investigations and streamflow analyses. A Monte Carlo analysis examined the performance of three techniques for estimating the moments and quantiles of a distribution using censored data sets. These techniques include a lognormal maximum likelihood estimator (MLE), a log-probability plot regression estimator, and a new log-partial probability-weighted moment estimator. Data sets were generated from a number of distributions commonly used to describe water quality and water quantity variables. A 'robust' fill-in method, which circumvents transformation bias in the real space moments, was implemented with all three estimation techniques to obtain a complete sample for computation of the sample mean and standard deviation. Regardless of the underlying distribution, the MLE generally performed as well as or better than the other estimators, though the moment and quantile estimators using all three techniques had comparable log-space root mean square errors (rmse) for censoring at or below the 20th percentile for samples sizes of n = 10, the 40th percentile for n = 25, and the 60th percentile for n = 50. Comparison of the log-space rmse and real- space rmse indicated that a log-space rmse was a better overall metric of estimator precision.},
author = {Kroll, Charles N. and Stedinger, Jery R.},
doi = {10.1029/95WR03294},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/estimation of moments and quantiles using censored data.pdf:pdf},
issn = {00431397},
journal = {Water Resources Research},
keywords = {http://dx.doi.org/10.1029/95WR03294, doi:10.1029/9},
number = {4},
pages = {1005--1012},
title = {{Estimation of moments and quantiles using censored data}},
volume = {32},
year = {1996}
}
@article{Gillom1984,
author = {Gllllom, Robert J and Kirsch, Robert M and Gilroy, Edward J and Survey, U S Geological},
doi = {10.1021/es00125a009},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Effect of Censoring Trace-Level Water-Quality Data on Trend-Detection Capability.pdf:pdf},
number = {1},
pages = {530--535},
title = {{Effect of Censoring Trace-Level Water-Quality Data Capability Trend-Detection}},
year = {1984}
}
@article{Berthouex1993,
abstract = {This paper presents a model that describes the precision of measurements at low concentrations (near the limit of detection). The total variance includes both background noise, which exists even when no analyte is present and is assumed to have a fixed variance, and analytical error, which in chemical measurements is often proportional to the concentrations of the analyte. This total variance model is used to define a limit of detection and two other measures of measurement precision. These are the Characteristic Limit, which is defined as the concentration where the variances of background noise and analytical error are equal, and the Limit of Guaranteed Purity, which defines how large the true concentration might be, in light of a given measured value. Data on lead in wastewater effluent and organic chemicals in sediment are used to illustrate the calculations and demonstrate the performance of the model.},
author = {Berthouex, P. M. and Gan, D. Robert},
doi = {10.2175/wer.65.6.8},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A{\_}Model{\_}Of{\_}Measurement{\_}Precision{\_}At{\_}Low{\_}Concentrations.pdf:pdf},
issn = {1061-4303},
journal = {Water Environment Research},
keywords = {ability to determine an,analyte in a sample,is based on a,lead,limit of detection,limit of purity,matrix,mdl,measurement,measurement error,method,organic chemicals,precision,regardless,s,the method detection limit,variance},
number = {6},
pages = {759--763},
title = {{A Model of Measurement Precision at Low Concentrations}},
volume = {65},
year = {1993}
}
@article{Ferguson2007,
abstract = {Analytical techniques for measuring trace arsenic (As) concentrations, such as the method described in this article, have much lower detection limits than conventional As measurement methods, and broader adoption of such methods could potentially drive a revision of the practical quantitation limit (PQL) for As in the future. Arsenic detection is limited by chloride interference in transmission quadrupole inductively coupled plasma/mass spectrometry (ICP/MS). However, high-resolution ICP/MS can measure As concentrations as low as 29 ng/L. The (nonenforceable) federal and California health standards for As are substantially lower than the PQL, so in the future a lower PQL could be a contributing factor when evaluating and reporting regulatory standards. These conclusions are likely to affect the decisions that water utility managers make about new and upgraded analytical equipment and treatment processes for As and other contaminants for which the PQL is currently greater than corresponding health standards. Water utility managers need to understand that the PQL is a moving target and incorporate that knowledge when planning for future analytical, reporting, and treatment operations.},
author = {Ferguson, Megan A. and Fernandez, Diego P. and Hering, Janet G.},
doi = {10.1002/j.1551-8833.2007.tb08010.x},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Lowering{\_}The{\_}Detection{\_}Limit{\_}For{\_}Arsenic{\_}Implications{\_}For{\_}A{\_}Future{\_}Practical{\_}Quantitation{\_}Limit.pdf:pdf},
issn = {0003150X},
journal = {Journal / American Water Works Association},
number = {8},
pages = {92--98},
title = {{Lowering the detection limit for arsenic: Implications for a future practical quantitation limit}},
volume = {99},
year = {2007}
}
@article{Shumway1989,
abstract = {The reporting procedures for potentially toxic pollutants are complicated by the fact that concentrations are measured using small samples that include a number of observations lying below some detection limit. Furthermore, there is often a small number of high concentrations observed in combination with a substantial number of low concentrations. This results in small, nonnormally distributed censored samples. This article presents maximum likelihood estimators for the mean of a population, based on censored samples that can be transformed to normality. The method estimates the optimal power transformation in the Box-Cox family by searching the censored-data likelihood. Maximum likelihood estimators for the mean in the transformed scale are calculated via the expectation-maximization algorithm. Estimates for the mean in the original scale are functions of the estimated mean and variance in the transformed population. Confidence intervals are computed using the delta method and the nonparametric percentile and bias-corrected percentile versions of Efron's bootstrap. A simulation study over sampling configurations expected with environmental data indicates that the delta method, combined with a reliable value for the power transformation, produces intervals with better coverage properties than the bootstrap intervals. {\textcopyright} 1989 Taylor {\&} Francis Group, LLC.},
author = {Shumway, R. H. and Azari, A. S. and Johnson, P.},
doi = {10.1080/00401706.1989.10488557},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimating Mean Concentrations under Transformation for Environmental Data with Detection Limits.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Bootstrap,Box-Cox transformation,Censored data,Delta method,EM algorithm,Maximum likelihood},
number = {3},
pages = {347--356},
title = {{Estimating mean concentrations under transformation for environmental data with detection limits}},
volume = {31},
year = {1989}
}
@book{Klein2003,
address = {New York},
author = {Klein, John P. and Moeschberge, Melvin L.},
edition = {2},
file = {:C$\backslash$:/Users/theto/Downloads/Survival analysis - Techniques for censored and truncated data.pdf:pdf},
isbn = {038795399X},
pages = {92--104},
publisher = {Springer-Verlag},
title = {{SURVIVAL ANALYSIS: Techniques for Censored and Truncated Data}},
year = {2003}
}
@article{She1997,
author = {She, Nian},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Analyzing Censored Water Quality Data Using a Nonparametric Approach -  Nian She.pdf:pdf},
number = {3},
title = {{ANALYZING CENSORED WATER QUALITY DATA USING A NON-PARAMETRIC APPROACh}},
volume = {33},
year = {1997}
}
@article{Yong2013,
author = {Yong, An Gie and Pearce, Sean},
doi = {10.1057/fsm.2014.17},
file = {:C$\backslash$:/Users/theto/Downloads/ABeginnersGuidetoFactorAnalysis{\_}FocusingonExploratoryFactorAnalysis.pdf:pdf},
issn = {14791846},
journal = {Tutorials in Quantitative Methods for Psychology},
number = {2},
pages = {79--94},
title = {{A Beginner's Guide to Factor Analysis: Focusing on Exploratory Factor Analysis}},
volume = {9},
year = {2013}
}
@article{Picco2016,
abstract = {Clustering is one of the main methods for getting insight on the underlying nature and structure of data. The purpose of clustering is organizing a set of data into clusters, such that the elements in each cluster are similar and different from those in other clusters. One of the most used clustering algorithms presently is K-means, because of its easiness for interpreting its results and implementation. The solution to the K-means clustering problem is NP-hard, which justifies the use of heuristic methods for its solution. To date, a large number of improvements to the algorithm have been proposed, ofwhich the most relevant were selected using systematic review methodology. As a result, 1125 documents on improvements were retrieved, and 79 were left after applying inclusion and exclusion criteria. The improvements selected were classified and summarized according to the algorithm steps: initializa- tion, classification, centroid calculation, and convergence. It is remarkable that some of the most successful algorithm variants were found. Some articles on trends in recent years were included, concerning K-means improvements and its use in other areas. Finally, it is considered that the main improvements may inspire the development of new heuristics for K-means or other clustering algorithms. Keywords:},
author = {Picco, Sergio and Villegas, Liliana and Tonelli, Franco and Merlo, Mario and Rigau, Javier and Diaz, Dario and Masuelli, Martin},
file = {:C$\backslash$:/Users/theto/Documents/git/STAT495F20-project-Ni/report/literature/the k-means algorithm evolution.pdf:pdf},
journal = {IntechOpen},
title = {{The K-Means Algorithm Evolution}},
url = {https://www.intechopen.com/books/advanced-biometric-technologies/liveness-detection-in-biometrics},
year = {2016}
}
@misc{EIP2020,
author = {{Environmental Integrity Project}},
file = {:C$\backslash$:/Users/theto/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/25d373d53644250e2b0e989f47940ec4b4fea84e.html:html;:C$\backslash$:/Users/theto/Documents/Thesis/literature/Coal Ash Groundwater Contamination Documenting Coal Ash Pollution.pdf:pdf},
title = {{Coal Ash Groundwater Contamination: Documenting Coal Ash Pollution}},
url = {https://environmentalintegrity.org/coal-ash-groundwater-contamination/},
year = {2020}
}
@misc{Threshold2020,
abstract = {Drinking water standards},
author = {{Environmental Protection Agency}},
file = {:C$\backslash$:/Users/theto/Documents/thesis{\_}backup/literature/npwdr{\_}complete{\_}table.pdf:pdf;:C$\backslash$:/Users/theto/Desktop/National Primary Drinking Water Regulations {\_} Ground Water and Drinking Water {\_} US EPA.pdf:pdf},
title = {{National Primary Drinking Water Regulations}},
url = {https://www.epa.gov/sites/production/files/2016-06/documents/npwdr{\_}complete{\_}table.pdf},
year = {2020}
}
@article{Gillespie2010,
author = {Gillespie, Brenda W and Chen, Qixuan and Reichert, Heidi and Franzblau, Alfred and Lepkowski, James and Adriaens, Peter and Demond, Avery and Luksemburg, William and Garabrant, David H and Gillespie, Brenda W and Chen, Qixuan and Reichert, Heidi and Elizabeth, Franzblau and Lepkowski, James and Adriaens, Peter and Demondf, Avery and Luksemburg, William and Garabrant, David H},
doi = {10.1097/EDE.0b013e3181ce9fD8},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimating{\_}Population{\_}Dist{\_}When{\_}Some{\_}Data{\_}Are{\_}Below{\_}A{\_}LOD{\_}Using{\_}Reverse{\_}Kaplan{\_}Meier{\_}Estimator.pdf:pdf},
journal = {Epidemiology},
title = {{Estimating Population Distributions When Some Data Are Below a Limit of Detection by Using a Reverse Kaplan-Meier Estimator}},
volume = {21},
year = {2010}
}
@article{Meijs2018,
author = {{Drew Hendrickson}, A T},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Imputation-Predicting-Missing-Values.pdf:pdf},
number = {July},
title = {{Missing Data Imputation: Predicting Missing Values}},
year = {2018}
}
@misc{Car2020,
author = {{Environmental Protection Agency}},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Disposal of Coal Combustion Residuals from Electric Utilities Rulemakings.pdf:pdf},
title = {{Disposal of Coal Combustion Residuals from Electric Utilities Rulemakings}},
url = {https://www.epa.gov/coalash/coal-ash-rule},
year = {2020}
}
@article{Lee2005,
author = {Lee, Lopaka and Helsel, Dennis},
doi = {10.1016/j.cageo.2005.03.012},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/statistical analysis of water quality data containing multiple detection limits.pdf:pdf},
keywords = {censored data,geochemistry,s},
pages = {1241--1248},
title = {{Statistical analysis of water-quality data containing multiple detection limits : S-language software for regression on order statistics {\$}}},
volume = {31},
year = {2005}
}
@article{Yavuz2017,
author = {Yavuz, Yasemin and Tekindal, Mustafa Agah and Dog, Beyza},
doi = {10.1007/s12539-015-0132-9},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/EvaluatingLeft-CensoredDataThr.pdf:pdf},
keywords = {left-censor {\'{a}} kaplan,limit of detection {\'{a}},limit of quantification,measurement uncertainty {\'{a}} regression,meier estimator {\'{a}},on order statistics {\'{a}}},
pages = {153--172},
title = {{Evaluating Left-Censored Data Through Substitution , Parametric , Semi-parametric , and Nonparametric Methods : A Simulation Study}},
year = {2017}
}
@article{Hewett2007,
abstract = {The purpose of this study was to compare the performance of several methods for statistically analyzing censored datasets [i.e. datasets that contain measurements that are less than the field limit-of-detection (LOD)] when estimating the 95th percentile and the mean of right-skewed occupational exposure data. The methods examined were several variations on the maximum likelihood estimation (MLE) and log-probit regression (LPR) methods, the common substitution methods, several non-parametric (NP) quantile methods for the 95th percentile and the NP Kaplan-Meier (KM) method. Each method was challenged with computer-generated censored datasets for a variety of plausible scenarios where the following factors were allowed to vary randomly within fairly wide ranges: the true geometric standard deviation, the censoring point or LOD and the sample size. This was repeated for both a single-laboratory scenario (i.e. single LOD) and a multiple-laboratory scenario (i.e. three LODs) as well as a single lognormal distribution scenario and a contaminated lognormal distribution scenario. Each method was used to estimate the 95th percentile and mean for the censored datasets (the NP quantile methods estimated only the 95th percentile). For each scenario, the method bias and overall imprecision (as indicated by the root mean square error or rMSE) were calculated for the 95th percentile and mean. No single method was unequivocally superior across all scenarios, although nearly all of the methods excelled in one or more scenarios. Overall, only the MLE- and LPR-based methods performed well across all scenarios, with the robust versions generally showing less bias than the standard versions when challenged with a contaminated lognormal distribution and multiple LODs. All of the MLE- and LPR-based methods were remarkably robust to departures from the lognormal assumption, nearly always having lower rMSE values than the NP methods for the exposure scenarios postulated. In general, the MLE methods tended to have smaller rMSE values than the LPR methods, particularly for the small sample size scenarios. The substitution methods tended to be strongly biased, but in some scenarios had the smaller rMSE values, especially for sample sizes {\textless}20. Surprisingly, the various NP methods were not as robust as expected, performing poorly in the contaminated distribution scenarios for both the 95th percentile and the mean. In conclusion, when using the rMSE rather than bias as the preferred comparison metric, the standard MLE method consistently outperformed the so-called robust variations of the MLE-based and LPR-based methods, as well as the various NP methods, for both the 95th percentile and the mean. When estimating the mean, the standard LPR method tended to outperform the robust LPR-based methods. Whenever bias is the main consideration, the robust MLE-based methods should be considered. The KM method, currently hailed by some as the preferred method for estimating the mean when the lognormal distribution assumption is questioned, did not perform well for either the 95th percentile or mean and is not recommended. {\textcopyright} The Author 2007. Published by Oxford University Press on behalf of the British Occupational Hygiene Society.},
author = {Hewett, Paul and Ganser, Gary H.},
doi = {10.1093/annhyg/mem045},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A Comparison of Several Methods for Analyzing Censored Data.pdf:pdf},
issn = {00034878},
journal = {Annals of Occupational Hygiene},
keywords = {Censored data analysis,Limit-of-detection},
number = {7},
pages = {611--632},
pmid = {17940277},
title = {{A comparison of several methods for analyzing censored data}},
volume = {51},
year = {2007}
}
@article{Elias1999,
author = {Elias, David and Goodman, Robert C},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/When{\_}Nothing{\_}Is{\_}Something{\_}Understanding{\_}Detection{\_}Limits.pdf:pdf},
number = {4},
pages = {519--521},
title = {{When Nothing Is Something : Understanding Detection Limits}},
volume = {13},
year = {1999}
}
