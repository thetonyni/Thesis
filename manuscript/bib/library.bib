Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Berthouex1993,
abstract = {This paper presents a model that describes the precision of measurements at low concentrations (near the limit of detection). The total variance includes both background noise, which exists even when no analyte is present and is assumed to have a fixed variance, and analytical error, which in chemical measurements is often proportional to the concentrations of the analyte. This total variance model is used to define a limit of detection and two other measures of measurement precision. These are the Characteristic Limit, which is defined as the concentration where the variances of background noise and analytical error are equal, and the Limit of Guaranteed Purity, which defines how large the true concentration might be, in light of a given measured value. Data on lead in wastewater effluent and organic chemicals in sediment are used to illustrate the calculations and demonstrate the performance of the model.},
author = {Berthouex, P. M. and Gan, D. Robert},
doi = {10.2175/wer.65.6.8},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A{\_}Model{\_}Of{\_}Measurement{\_}Precision{\_}At{\_}Low{\_}Concentrations.pdf:pdf},
issn = {1061-4303},
journal = {Water Environment Research},
keywords = {ability to determine an,analyte in a sample,is based on a,lead,limit of detection,limit of purity,matrix,mdl,measurement,measurement error,method,organic chemicals,precision,regardless,s,the method detection limit,variance},
number = {6},
pages = {759--763},
title = {{A Model of Measurement Precision at Low Concentrations}},
volume = {65},
year = {1993}
}
@article{Elias1999,
author = {Elias, David and Goodman, Robert C},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/When{\_}Nothing{\_}Is{\_}Something{\_}Understanding{\_}Detection{\_}Limits.pdf:pdf},
number = {4},
pages = {519--521},
title = {{When Nothing Is Something : Understanding Detection Limits}},
volume = {13},
year = {1999}
}
@book{Clark2013,
abstract = {The purpose of this document is to provide a conceptual introduction to statistical or machine learning (ML) techniques for those that might not normally be exposed to such approaches during their required typical statistical training.},
author = {Clark, Michael},
booktitle = {An Introduction to Machine Learning with Applications in R},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/ML{\_}inR.pdf:pdf},
keywords = {data minin,machine learning,statistical learning},
pages = {1--43},
title = {{An Introduction to Machine Learning with Applications in R}},
year = {2013}
}
@article{He2020,
abstract = {Measures of substance concentration in urine, serum or other biological matrices often have an assay limit of detection. When concentration levels fall below the limit, the exact measures cannot be obtained. Instead, the measures are censored as only partial information that the levels are under the limit is known. Assuming the concentration levels are from a single population with a normal distribution or follow a normal distribution after some transformation, Tobit regression models, or censored normal regression models, are the standard approach for analyzing such data. However, in practice, it is often the case that the data can exhibit more censored observations than what would be expected under the Tobit regression models. One common cause is the heterogeneity of the study population, caused by the existence of a latent group of subjects who lack the substance measured. For such subjects, the measurements will always be under the limit. If a censored normal regression model is appropriate for modeling the subjects with the substance, the whole population follows a mixture of a censored normal regression model and a degenerate distribution of the latent class. While there are some studies on such mixture models, a fundamental question about testing whether such mixture modeling is necessary, i.e. whether such a latent class exists, has not been studied yet. In this paper, three tests including Wald test, likelihood ratio test and score test are developed for testing the existence of such latent class. Simulation studies are conducted to evaluate the performance of the tests, and two real data examples are employed to illustrate the tests.},
author = {He, Hua and Tang, Wan and Kelly, Tanika and Li, Shengxu and He, Jiang},
doi = {10.1177/0962280219885985},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Statistical{\_}Tests{\_}For{\_}Latent{\_}Class{\_}In{\_}Censored{\_}Data{\_}Due{\_}To{\_}Detection{\_}Limit.pdf:pdf},
issn = {14770334},
journal = {Statistical Methods in Medical Research},
keywords = {Censored normal regression,Tobit model,Wald test,detection limit,latent class,likelihood ratio test,mixture Tobit model,score test},
number = {8},
pages = {2179--2197},
pmid = {31736411},
title = {{Statistical tests for latent class in censored data due to detection limit}},
volume = {29},
year = {2020}
}
@misc{Qualicum2007,
address = {British Columbia},
author = {{Parksville Qualicum News}},
file = {:C$\backslash$:/Users/theto/Downloads/Groundwater{\_}what{\_}is{\_}it{\_}{\_}Parksville{\_}Qualicum{\_}News{\_}British{\_}Columbia{\_}Canada{\_}{\_}{\_}November{\_}23{\_}2007{\_}{\_}p0a23.pdf:pdf},
month = {nov},
title = {{Groundwater: What Is It?}},
year = {2007}
}
@article{Meijs2018,
author = {{Drew Hendrickson}, A T},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Imputation-Predicting-Missing-Values.pdf:pdf},
number = {July},
title = {{Missing Data Imputation: Predicting Missing Values}},
year = {2018}
}
@book{Bishop2006,
address = {Singapore},
author = {Bishop, Christopher},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:pdf},
isbn = {9780387310732},
publisher = {Springer Science and Business Media},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Glass2001,
abstract = {A retrospective assessment of exposure to benzene was carried out for a nested case control study of lympho-haematopoietic cancers, including leukaemia, in the Australian petroleum industry. Each job or task in the industry was assigned a Base Estimate (BE) of exposure derived from task-based personal exposure assessments carried out by the company occupational hygienists. The BEs corresponded to the estimated arithmetic mean exposure to benzene for each job or task and were used in a deterministic algorithm to estimate the exposure of subjects in the study. Nearly all of the data sets underlying the BEs were found to contain some values below the limit of detection (LOD) of the sampling and analytical methods and some were very heavily censored; up to 95{\%} of the data were below the LOD in some data sets. It was necessary, therefore, to use a method of calculating the arithmetic mean exposures that took into account the censored data. Three different methods were employed in an attempt to select the most appropriate method for the particular data in the study. A common method is to replace the missing (censored) values with half the detection limit. This method has been recommended for data sets where much of the data are below the limit of detection or where the data are highly skewed; with a geometric standard deviation of 3 or more. Another method, involving replacing the censored data with the limit of detection divided by the square root of 2, has been recommended when relatively few data are below the detection limit or where data are not highly skewed. A third method that was examined is Cohen's method. This involves mathematical extrapolation of the left-hand tail of the distribution, based on the distribution of the uncensored data, and calculation of the maximum likelihood estimate of the arithmetic mean. When these three methods were applied to the data in this study it was found that the first two simple methods give similar results in most cases. Cohen's method on the other hand, gave results that were generally, but not always, higher than simpler methods and in some cases gave extremely high and even implausible estimates of the mean. It appears that if the data deviate substantially from a simple log-normal distribution, particularly if high outliers are present, then Cohen's method produces erratic and unreliable estimates. After examining these results, and both the distributions and proportions of censored data, it was decided that the half limit of detection method was most suitable in this particular study. {\textcopyright} 2001 British Occupational Hygiene Society.},
author = {Glass, D. C. and Gray, C. N.},
doi = {10.1016/S0003-4878(01)00022-9},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimating{\_}Mean{\_}Exposures{\_}From{\_}Censored{\_}Data.pdf:pdf},
issn = {00034878},
journal = {Annals of Occupational Hygiene},
keywords = {Benzene,Censored data,Epidemiology,Limit of detection,Retrospective exposure assessment},
number = {4},
pages = {275--282},
pmid = {11378148},
title = {{Estimating mean exposures from censored data: Exposure to benzene in the Australian petroleum industry}},
volume = {45},
year = {2001}
}
@misc{EIP2020,
author = {{Environmental Integrity Project}},
file = {:C$\backslash$:/Users/theto/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/25d373d53644250e2b0e989f47940ec4b4fea84e.html:html;:C$\backslash$:/Users/theto/Documents/Thesis/literature/Coal Ash Groundwater Contamination Documenting Coal Ash Pollution.pdf:pdf},
title = {{Coal Ash Groundwater Contamination: Documenting Coal Ash Pollution}},
url = {https://environmentalintegrity.org/coal-ash-groundwater-contamination/},
urldate = {07-26-2020},
year = {2020}
}
@misc{Car2020,
author = {{Environmental Protection Agency}},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Disposal of Coal Combustion Residuals from Electric Utilities Rulemakings.pdf:pdf},
title = {{Disposal of Coal Combustion Residuals from Electric Utilities Rulemakings}},
url = {https://www.epa.gov/coalash/coal-ash-rule},
year = {2020}
}
@article{Hewett2007,
abstract = {The purpose of this study was to compare the performance of several methods for statistically analyzing censored datasets [i.e. datasets that contain measurements that are less than the field limit-of-detection (LOD)] when estimating the 95th percentile and the mean of right-skewed occupational exposure data. The methods examined were several variations on the maximum likelihood estimation (MLE) and log-probit regression (LPR) methods, the common substitution methods, several non-parametric (NP) quantile methods for the 95th percentile and the NP Kaplan-Meier (KM) method. Each method was challenged with computer-generated censored datasets for a variety of plausible scenarios where the following factors were allowed to vary randomly within fairly wide ranges: the true geometric standard deviation, the censoring point or LOD and the sample size. This was repeated for both a single-laboratory scenario (i.e. single LOD) and a multiple-laboratory scenario (i.e. three LODs) as well as a single lognormal distribution scenario and a contaminated lognormal distribution scenario. Each method was used to estimate the 95th percentile and mean for the censored datasets (the NP quantile methods estimated only the 95th percentile). For each scenario, the method bias and overall imprecision (as indicated by the root mean square error or rMSE) were calculated for the 95th percentile and mean. No single method was unequivocally superior across all scenarios, although nearly all of the methods excelled in one or more scenarios. Overall, only the MLE- and LPR-based methods performed well across all scenarios, with the robust versions generally showing less bias than the standard versions when challenged with a contaminated lognormal distribution and multiple LODs. All of the MLE- and LPR-based methods were remarkably robust to departures from the lognormal assumption, nearly always having lower rMSE values than the NP methods for the exposure scenarios postulated. In general, the MLE methods tended to have smaller rMSE values than the LPR methods, particularly for the small sample size scenarios. The substitution methods tended to be strongly biased, but in some scenarios had the smaller rMSE values, especially for sample sizes {\textless}20. Surprisingly, the various NP methods were not as robust as expected, performing poorly in the contaminated distribution scenarios for both the 95th percentile and the mean. In conclusion, when using the rMSE rather than bias as the preferred comparison metric, the standard MLE method consistently outperformed the so-called robust variations of the MLE-based and LPR-based methods, as well as the various NP methods, for both the 95th percentile and the mean. When estimating the mean, the standard LPR method tended to outperform the robust LPR-based methods. Whenever bias is the main consideration, the robust MLE-based methods should be considered. The KM method, currently hailed by some as the preferred method for estimating the mean when the lognormal distribution assumption is questioned, did not perform well for either the 95th percentile or mean and is not recommended. {\textcopyright} The Author 2007. Published by Oxford University Press on behalf of the British Occupational Hygiene Society.},
author = {Hewett, Paul and Ganser, Gary H.},
doi = {10.1093/annhyg/mem045},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A Comparison of Several Methods for Analyzing Censored Data.pdf:pdf},
issn = {00034878},
journal = {Annals of Occupational Hygiene},
keywords = {Censored data analysis,Limit-of-detection},
number = {7},
pages = {611--632},
pmid = {17940277},
title = {{A comparison of several methods for analyzing censored data}},
volume = {51},
year = {2007}
}
@article{Cohen-addad2019,
author = {Cohen-addad, Vincent and Universit{\'{e}}, Sorbonne},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/ContentServer.pdf:pdf},
number = {4},
title = {{Hierarchical Clustering : Objective Functions and Algorithms}},
volume = {66},
year = {2019}
}
@article{Ferguson2007,
abstract = {Analytical techniques for measuring trace arsenic (As) concentrations, such as the method described in this article, have much lower detection limits than conventional As measurement methods, and broader adoption of such methods could potentially drive a revision of the practical quantitation limit (PQL) for As in the future. Arsenic detection is limited by chloride interference in transmission quadrupole inductively coupled plasma/mass spectrometry (ICP/MS). However, high-resolution ICP/MS can measure As concentrations as low as 29 ng/L. The (nonenforceable) federal and California health standards for As are substantially lower than the PQL, so in the future a lower PQL could be a contributing factor when evaluating and reporting regulatory standards. These conclusions are likely to affect the decisions that water utility managers make about new and upgraded analytical equipment and treatment processes for As and other contaminants for which the PQL is currently greater than corresponding health standards. Water utility managers need to understand that the PQL is a moving target and incorporate that knowledge when planning for future analytical, reporting, and treatment operations.},
author = {Ferguson, Megan A. and Fernandez, Diego P. and Hering, Janet G.},
doi = {10.1002/j.1551-8833.2007.tb08010.x},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Lowering{\_}The{\_}Detection{\_}Limit{\_}For{\_}Arsenic{\_}Implications{\_}For{\_}A{\_}Future{\_}Practical{\_}Quantitation{\_}Limit.pdf:pdf},
issn = {0003150X},
journal = {Journal / American Water Works Association},
number = {8},
pages = {92--98},
title = {{Lowering the detection limit for arsenic: Implications for a future practical quantitation limit}},
volume = {99},
year = {2007}
}
@article{Canales2018,
author = {Canales, Robert},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Methods-For-Handling-Left-Censored-Data-in-Quantitative.pdf:pdf},
keywords = {accepted 8 august,left censored,limit of detection,quantitative microbial risk assessment,received 18 may 2018},
number = {20},
pages = {1--10},
title = {{Methods for Handling Left-Censored Data in Quantitative Microbial Risk Assessment}},
volume = {84},
year = {2018}
}
@article{Akard2002,
abstract = {With the need for emission measurements of super ultra low emission vehicles (SULEV), analyzer manufacturers have been required to produce more precise and accurate analyzers. In order to compare analyzers, the customer must understand the different specifications used by the analyzer manufacturers. One specification that some manufacturers have used is the limit of detection (LOD) to indicate the reliability of the analyzer output at low concentrations. There are various methods for determining the LOD for a given analyzer. The authors will demonstrate how variations in methodology can produce different LOD values for a specific analyzer and what it means for the automotive emission analyzers. It is also demonstrated that the standard deviations of a zero signal, which is related to LOD, can be heavily influenced by data processing, such as data length in use and/or data smoothing. The LOD values obtained will be compared to the limit of quantification (LOQ) for that analyzer. In order to evaluate the utility of LOD measurements, various analyzer performances will be modeled and evaluated. While the LOD is a valid statistical approach to determining the presence of a component in a sample matrix, it is not directly applicable to emissions measurements. A discussion about the emission measurement requirements will demonstrate the limited applicability of this analyzer specification. Alternative methods for comparing analyzer specifications will be presented and justified. Copyright {\textcopyright} 2002 Society of Automotive Engineers, Inc.},
author = {Akard, Michael and Tsurumi, Kazuya and Oestergaard, Karl and Inoue, Kaori},
doi = {10.4271/2002-01-2711},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Why{\_}The{\_}LOD{\_}Is{\_}Not{\_}Appropriate{\_}Spec{\_}for{\_}Automotive{\_}Emissions.pdf:pdf},
issn = {26883627},
journal = {SAE Technical Papers},
number = {2002},
pages = {1321--1328},
title = {{Why the Limit of Detection (LOD) Value is Not an Appropriate Specification for Automotive Emissions Analyzers}},
volume = {111},
year = {2002}
}
@article{Poulos2018,
abstract = {Missing data imputation can help improve the performance of prediction models in situations where missing data hide useful information. This paper compares methods for imputing missing categorical data for supervised classification tasks. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different levels of additional missing-data perturbation. We show imputation methods can increase predictive accuracy in the presence of missing-data perturbation, which can actually improve prediction accuracy by regularizing the classifier. We achieve results comparable to the state-of-the-art on the Adult dataset with missing-data perturbation and k-nearest-neighbors (k-NN) imputation.},
archivePrefix = {arXiv},
arxivId = {1610.09075},
author = {Poulos, Jason and Valle, Rafael},
doi = {10.1080/08839514.2018.1448143},
eprint = {1610.09075},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Imputation-For-Supervised-Learning.pdf:pdf},
issn = {10876545},
journal = {Applied Artificial Intelligence},
keywords = {at http,at https,com,decision trees,github,imputation methods,io,jvpoulos,mdi,mdi-supp,missing data,neural networks,papers,pdf,perturbation,rafaelvalle,random forests,supplementary material is available,the code used for,the online,this project is available,we thank},
number = {2},
pages = {186--196},
title = {{Missing Data Imputation for Supervised Learning}},
volume = {32},
year = {2018}
}
@misc{Threshold2020,
abstract = {Drinking water standards},
author = {{Environmental Protection Agency}},
file = {:C$\backslash$:/Users/theto/Documents/thesis{\_}backup/literature/npwdr{\_}complete{\_}table.pdf:pdf;:C$\backslash$:/Users/theto/Desktop/National Primary Drinking Water Regulations {\_} Ground Water and Drinking Water {\_} US EPA.pdf:pdf},
title = {{National Primary Drinking Water Regulations}},
url = {https://www.epa.gov/sites/production/files/2016-06/documents/npwdr{\_}complete{\_}table.pdf},
urldate = {2020},
year = {2020}
}
@article{Gillespie2010,
author = {Gillespie, Brenda W and Chen, Qixuan and Reichert, Heidi and Franzblau, Alfred and Lepkowski, James and Adriaens, Peter and Demond, Avery and Luksemburg, William and Garabrant, David H and Gillespie, Brenda W and Chen, Qixuan and Reichert, Heidi and Elizabeth, Franzblau and Lepkowski, James and Adriaens, Peter and Demondf, Avery and Luksemburg, William and Garabrant, David H},
doi = {10.1097/EDE.0b013e3181ce9fD8},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimating{\_}Population{\_}Dist{\_}When{\_}Some{\_}Data{\_}Are{\_}Below{\_}A{\_}LOD{\_}Using{\_}Reverse{\_}Kaplan{\_}Meier{\_}Estimator.pdf:pdf},
journal = {Epidemiology},
title = {{Estimating Population Distributions When Some Data Are Below a Limit of Detection by Using a Reverse Kaplan-Meier Estimator}},
volume = {21},
year = {2010}
}
@article{Berthouex2020,
author = {Berthouex, Paul},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A{\_}Study{\_}Of{\_}The{\_}Precision{\_}Of{\_}Lead{\_}...{\_}Near{\_}The{\_}Method{\_}LOD.pdf:pdf},
keywords = {are expected either to,be absent or to,bias,focus on chemicals that,in environmental quality,lead,limit of detection,many important scientific problems,monitoring,precision},
number = {5},
pages = {620--629},
title = {{A Study of the Precision of Lead Measurements at Concentrations Near the Method Limit of Detection}},
volume = {65},
year = {2020}
}
@article{Shumway1989,
abstract = {The reporting procedures for potentially toxic pollutants are complicated by the fact that concentrations are measured using small samples that include a number of observations lying below some detection limit. Furthermore, there is often a small number of high concentrations observed in combination with a substantial number of low concentrations. This results in small, nonnormally distributed censored samples. This article presents maximum likelihood estimators for the mean of a population, based on censored samples that can be transformed to normality. The method estimates the optimal power transformation in the Box-Cox family by searching the censored-data likelihood. Maximum likelihood estimators for the mean in the transformed scale are calculated via the expectation-maximization algorithm. Estimates for the mean in the original scale are functions of the estimated mean and variance in the transformed population. Confidence intervals are computed using the delta method and the nonparametric percentile and bias-corrected percentile versions of Efron's bootstrap. A simulation study over sampling configurations expected with environmental data indicates that the delta method, combined with a reliable value for the power transformation, produces intervals with better coverage properties than the bootstrap intervals. {\textcopyright} 1989 Taylor {\&} Francis Group, LLC.},
author = {Shumway, R. H. and Azari, A. S. and Johnson, P.},
doi = {10.1080/00401706.1989.10488557},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimating Mean Concentrations under Transformation for Environmental Data with Detection Limits.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Bootstrap,Box-Cox transformation,Censored data,Delta method,EM algorithm,Maximum likelihood},
number = {3},
pages = {347--356},
title = {{Estimating mean concentrations under transformation for environmental data with detection limits}},
volume = {31},
year = {1989}
}
@book{Lantz2013,
address = {Birmingham},
author = {Lantz, Brett},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/MachineLearningR{\_}{\_}Brett{\_}Lantz.pdf:pdf},
isbn = {9781782162148},
publisher = {Packt Publishing},
title = {{Machine Learning with R}},
year = {2013}
}
@article{Kelderman2019,
author = {Kelderman, Keene and Kunstman, Ben and Roy, Hayley and Sivakumar, Namratha and Mccormick, Samantha and Bernhardt, Courtney},
file = {:C$\backslash$:/Users/theto/Documents/harvard-summer-biostats/literature/National-Coal-Ash-Report-3.4.19-1.pdf:pdf},
title = {{Coal's Poisonous Legacy: Groundwater Contaminated by Coal Ash Across the U.S.}},
year = {2019}
}
@article{Chen2011,
author = {Chen, Haiying and Quandt, Sara A and Grzywacz, Joseph G and Arcury, Thomas A and Environmental, Source and Perspectives, Health and March, No and Chen, Haiying and Quandi, Sara A and Grzywacz, Joseph G and Arcury, Thomas A},
doi = {10.1289/ehp.l002124},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/A{\_}Distribution{\_}Based{\_}Multiple{\_}Imputation{\_}Method{\_}For{\_}Handling{\_}...{\_}Values{\_}Below{\_}The{\_}LOD.pdf:pdf},
number = {3},
pages = {351--356},
title = {{A Distribution-Based Multiple Imputation Method for Handling Bivariate Pesticide Data with Values below the Limit of Detection}},
volume = {119},
year = {2011}
}
@article{Slymen1994,
abstract = {Values below a specified detection limit are a common occurrence in environmental measurement which complicates statistical analysis. This paper addresses the problem of how these data may be included in hypothesis testing and demonstrates a solution that uses a regression model in the SAS statistical package with the ability to accommodate left-censored data. To illustrate, the LIFEREG procedure is applied to data for an analyte (tin) found in fish tissue during a multifactorial bioaccumulation study involving two experimental sites, three experiments at each site, and sampling on multiple days. The results of this analysis are presented, indicating evidence of bioaccumulation but no significant differences between sites or experiments. While regression methods to accommodate left-censored data are not new, their use in environmental studies does not appear to be widely practiced. The strengths and limitations of this approach are discussed. {\textcopyright} 1994, American Chemical Society. All rights reserved.},
author = {Slymen, Donald J. and {De Peyster}, Ann and Donohoe, Regina R.},
doi = {10.1021/es00054a022},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Hypothesis Testing with Values below Detection Limit in Environmental Studies.pdf:pdf},
issn = {15205851},
journal = {Environmental Science and Technology},
number = {5},
pages = {898--902},
title = {{Hypothesis Testing with Values below Detection Limit in Environmental Studies}},
volume = {28},
year = {1994}
}
@article{Byer2019,
author = {Byer, David and Carlson, Kenneth H},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Real{\_}Time{\_}Detection{\_}Of{\_}Intentional{\_}Chemical{\_}Contamination{\_}In{\_}The{\_}Distribution{\_}System.pdf:pdf},
number = {7},
pages = {130--133},
title = {{Real-time detection of intentional chemical contamination in the distributional system}},
volume = {97},
year = {2019}
}
@article{Lafleur2011,
abstract = {In analytic chemistry a detection limit (DL) is the lowest measurable amount of an analyte that can be distinguished from a blank; many biomedical measurement technologies exhibit this property. From a statistical perspective, these data present inferential challenges because instead of precise measures, one only has information that the value is somewhere between 0 and the DL (below detection limit, BDL). Substitution of BDL values, with 0 or the DL can lead to biased parameter estimates and a loss of statistical power. Statistical methods that make adjustments when dealing with these types of data, often called left-censored data, are available in many commercial statistical packages. Despite this availability, the use of these methods is still not widespread in biomedical literature. We have reviewed the statistical approaches of dealing with BDL values, and used simulations to examine the performance of the commonly used substitution methods and the most widely available statistical methods. We have illustrated these methods using a study undertaken at the Vanderbilt-Ingram Cancer Center, to examine the serum bile acid levels in patients with colorectal cancer and adenoma. We have found that the modern methods for BDL values identify disease-related differences that are often missed, with statistically naive approaches.},
author = {Lafleur, Bonnie and Lee, Wooin and Billhiemer, Dean and Lockhart, Craig and Liu, Junmei and Merchant, Nipun},
doi = {10.4103/1477-3163.79681},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Statistical methods for assays with limits of detection{\_} Serum bile acid as a differentiator between patients with normal colons, adenomas, and colorectal cancer.pdf:pdf},
issn = {09746773},
journal = {Journal of Carcinogenesis},
keywords = {Bile acids,colorectal cancer,detection limits,statistical methods},
pages = {1--8},
pmid = {21712958},
title = {{Statistical methods for assays with limits of detection: Serum bile acid as a differentiator between patients with normal colons, adenomas, and colorectal cancer}},
volume = {10},
year = {2011}
}
@article{Gilliom1986,
abstract = {A recurring difficulty encountered in investigations of many metals and organic contaminants in ambient waters is that a substantial portion of water sample concentrations are below limits of detection established by analytical laboratories. Several methods were evaluated for estimating distributional parameters for such censored data sets using only uncensored observations. Their reliabilities were evaluated by a Monte Carlo experiment in which small samples were generated from a wide range of parent distributions and censored at varying levels. Eight methods were used to estimate the mean, standard deviation, median, and interquartile range. Criteria were developed, based on the distribution of uncensored observations, for determining the best performing parameter estimation method for any particular data set. The most robust method for minimizing error in censored‐sample estimates of the four distributional parameters over all simulation conditions was the log‐probability regression method. With this method, censored observations are assumed to follow the zero‐to‐censoring level portion of a lognormal distribution obtained by a least squares regression between logarithms of uncensored concentration observations and their z scores. When method performance was separately evaluated for each distributional parameter over all simulation conditions, the log‐probability regression method still had the smallest errors for the mean and standard deviation, but the lognormal maximum likelihood method had the smallest errors for the median and interquartile range. When data sets were classified prior to parameter estimation into groups reflecting their probable parent distributions, the ranking of estimation methods was similar, but the accuracy of error estimates was markedly improved over those without classification. This paper is not subject to U.S. copyright. Published in 1986 by the American Geophysical Union.},
author = {Gilliom, Robert J. and Helsel, Dennis R.},
doi = {10.1029/WR022i002p00135},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimation of Distributional Parameters for Censored Trace Level Water Quality Data Estimation Techniques.pdf:pdf},
issn = {19447973},
journal = {Water Resources Research},
number = {2},
pages = {135--146},
title = {{Estimation of Distributional Parameters for Censored Trace Level Water Quality}},
volume = {22},
year = {1986}
}
@article{May2012,
abstract = {Data subject to detection limits appear in a wide variety of studies. Data subject to detection limits are usually left-censored at the detection limit, often due to limitations in the measurement procedure being used. This thesis addresses three issues common to the analysis of data subject to detection limits. The first of these is the estimation of the limit of detection using repeated measurements from known analyte concentrations. An innovative change-point model is proposed to more accurately model the standard deviation of measured analyte concentrations, resulting in improved estimation of the limit of detection. The pro- posed methodology is applied to copy number data from an HIV pilot study. The second topic concerns estimation using generalized linear models when multiple covariates are subject to a limit of detection. We propose a Monte Carlo version of the EM algorithm similar to that in Ibrahim, Lipsitz, and Chen to handle a large number of covariates subject to detection limits in generalized linear models. Censored covariate values are sampled using the Adaptive Rejec- tion Metropolis Algorithm of Gilks, Best, and Tan. This procedure is applied to data from the National Health and Nutrition Examination Survey (NHANES), in which values of urinary heavy metals are subject to a limit of detection. Through simulation studies, we show that the proposed approach can lead to a significant reduction in variance for parameter estimates in these models, improving the power of such studies. The third and final topic addresses the joint modeling of longitudinal and survival data using time-varying covariates that are both intermittently missing and subject to a limit of detection. The model is motivated by data from the Multicenter AIDS Cohort Study (MACS), in which HIV+ subjects have viral load and CD4 cell counts measured at repeated visits along with survival data. The viral load data is subject to both left-censoring due to detection limits (17{\%}) and intermittent missing- ness (27{\%}). A Bayesian analysis is conducted on the MACS data using the proposed joint model. The proposed method is shown to improve the precision of estimates when compared to alternative methods.},
author = {May, Ryan C},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Estimation{\_}Methods{\_}for{\_}Data{\_}Subject{\_}to{\_}Detection{\_}Limits.pdf:pdf},
pages = {82},
title = {{Estimation Methods for Data Subject to Detection Limits}},
year = {2012}
}
@misc{Pohlmann92,
author = {Pohlmann, K. F.},
file = {:C$\backslash$:/Users/theto/Downloads/groundwaterissue{\_}errors{\_}sampling.pdf:pdf},
publisher = {EPA},
title = {{Potential Sources of Error in Groundwater Sampling at Hazardous Waste Sites}},
year = {1992}
}
@article{Eloul2015,
abstract = {We derive approximate expressions for the average number of diffusive impacts/hits of nanoparticles on microdisc and microwire electrodes for the case where the impact leads to the loss of the nanoparticles from solution either via irreversible adsorption or complete electro-dissolution. The theory can also be applied to sub-micrometre size electrodes (nano-electrodes). The resulting equations can be utilised to analyse the number of impacts and its variance in the 'nano-impact' experiment. We also provide analytical expressions for the first passage time of an impact for dilute nanoparticle solutions in the continuum limit of Fickian diffusion. The expressions for the first passage times are used to estimate the lower limit of detection in ultra-dilute nanoparticle solutions for typical nano-impact experiments, and show the advantage of using microwire electrodes in ultra-dilute solutions or solutions containing larger nano-particles.},
author = {Eloul, Shaltiel and K{\"{a}}telh{\"{o}}n, Enno and Batchelor-McAuley, Christopher and Tschulik, Kristina and Compton, Richard G.},
doi = {10.1016/j.jelechem.2015.07.042},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/Diffusional{\_}Impacts{\_}Of{\_}Nanaparticles...{\_}The{\_}Limit{\_}Of{\_}Detection{\_}and{\_}First{\_}Passage{\_}Statistics.pdf:pdf},
issn = {15726657},
journal = {Journal of Electroanalytical Chemistry},
keywords = {Microcylinder,Microdisc,Microwire electrode,Nano-impacts,Nanoparticle detection,Nanoparticle voltammetry,Ultra-low concentration,Wire electrode},
pages = {136--142},
publisher = {Elsevier B.V.},
title = {{Diffusional impacts of nanoparticles on microdisc and microwire electrodes: The limit of detection and first passage statistics}},
url = {http://dx.doi.org/10.1016/j.jelechem.2015.07.042},
volume = {755},
year = {2015}
}
@article{Graham2009,
abstract = {This review presents a practical summary of the missing data literature, including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. Practical missing data analysis issues are discussed, most notably the inclusion of auxiliary variables for improving power and reducing bias. Solutions are given for missing data challenges such as handling longitudinal, categorical, and clustered data with normal-model MI; including interactions in the missing data model; and handling large numbers of variables. The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. Strategies suggested for reducing attrition bias include using auxiliary variables, collecting follow-up data on a sample of those initially missing, and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.},
author = {Graham, John W.},
doi = {10.1146/annurev.psych.58.110405.085530},
file = {:C$\backslash$:/Users/theto/Documents/thesis/literature/Missing-Data-Analysis-Making-It-Work-In-The-Real-World.pdf:pdf},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {attrition,maximum likelihood,missingness,multiple imputation,nonignorable,planned missingness},
number = {1},
pages = {549--576},
title = {{Missing Data Analysis: Making It Work in the Real World}},
volume = {60},
year = {2009}
}
@article{Ohio2007,
author = {{Environmental Protection Agency}},
file = {:C$\backslash$:/Users/theto/Documents/Thesis/literature/TGM-5.pdf:pdf},
title = {{Monitoring Well Placement}},
year = {2007}
}
