---
title: "simulation-notes"
author: "Tony Ni"
date: "10/5/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Feedback (on generating-missing-data.Rmd)

* want to preserve the truth, dont want to overwrite value column

* BUILD UP dataset instead of making new one each time so we can compare in the end

* want to repeat this process multiple times when i set up simulation study (do this at a larger scale)

* read up on how to do simulation studies (prof baileys github) here: https://github.com/bebailey/research-tutorials/tree/master/04_coding-simulation

* what do we the structure to be like in the end? 

* THINKING OF EXPLCIIT NAMING SCHEMES INSTEAD OF JUST MY_DF# AND 'value' (think of what the names should be)

* when we are generating data, the ideal simulation set up will be setting up a df, and then that df should be able to be put into the functions w/o any changes

* rerun these functions MULTIPLE times repeatedly with new datasets (does it do better if we assume lognormal dist vs. normal dist? how closely do these methods get to the actual statistics?)

* create a function to apply all of these functions to the generated data

* FOR NEXT WEEK: focus on understanding what a simulation study IS -- put on pause the codework

* read the morris paper on simulation studies and then thinking about what it is i want to show with this simulation study (there is the big picture: comparison of how these methods perform vs the truth BUT what are we making these comparisons based off of, how do we know substitution method is better/worse etc. HOW DO WE COMPARE THE ACTUAL DISTRIBUTION OF VALUES -- usually just summary statistics... we might look at overall mean and median of values or summary statistics based off on whether or not if they are LOD values -- how does avg values compare btwn methods)

* after getting better sense of larger simulation study might look like -- will have to go back into code (go back into lnorm and making sure that what you're inputting is what the function is asking for, the parameters that r takes in is not always what wikipedia takes in -- verify parameters)

* verifying what the functions are outputting, can you get the rawdata, if so how? if not, are there anyways where we can? 

--

* when we know methods are dependent, can we use these methods that assume independence?

* in this world, we assume measurements are independent (from random sample), we are trying to figure out how we can fill in info from LOD in this dataset to estimate the average concentration despite having this LOD problems

* make new chapter 1 (not with water in the future)

* performance metrics -- if you know method is performing well if when you generate data based on mean of 5 and use methods to fill in values, when you repeatedly the method, you get values close to 5, that your empirical/simulated mean is about 5

* coverage, and confidence intervals could useful!

* start small!!!!! before trying to scale up!

### ADEMPS

#### Aims

* Q: what are the aims of the study?

(Hewett and Ganser, 2007) conducted a simulation study and discuss that it is very unlikely for one method to outperform another method. As such, the aims of our simulation study are not to find the "best," (in terms of whatever performance metric we are interested in). Instead, my goal is to assess the performance of each method in cases of different percentages of censoring and different sample sizes -- and seeing which methods perform better or worse than the others for their specific censoring percentage and sample size. 

#### Data-Generating Mechanisms

* Q: resampling vs. simulation from parametric dist?
* Q: how simple/complex should the model be?
* Q: should it be based on real data?
* Q: which factors to vary? which levels of factors to use?

* A: GENERAL, EXPAND LATER) Bolks (2014) discuss how different methods have different performances with varying sample sizes and censoring rates.  As such, we resampling from a simple lognormal distribution of sample sizes of 10, 100, and 1000 AND different censoring rates of 10%, 30%, and 50%.

* A: I will be generating data from a log-normal distribution with the parameters of mean, $\mu$ = 1 and standard deviation, $\sigma$ = 0.5.

```{r}
#playing around with log-normal dist.
m <- 1
s<- 0.5

vector <- rlnorm(10000, log(m^2 / sqrt(s^2 + m^2)), 
                   sdlog=sqrt(log(1 + (s^2 / m^2))))
mean(vector)
sd(vector)
```

#### Estimand

* Q: define estimands of the simulation study

* A: the estimands (variable which is to be estimated in a statistical analysis) in our study are going to be the parameters of the log-normal distribution, $\mu$ and $\sigma$. 

#### Methods

* Q: identify methods to be evaluated; are they appropriate?

* A: we will run our simulation a set number of times (10000?) for each simulated dataset(s) (with varying sample sizes and percentages of censoring for left censored data). 

within each run, we will use following 4 methods (substitution, MLE, kaplan-meier, and ROS) to obtained censored values in order to compute the sample mean $\bar{X}$ and sample sd $s$ for each respective sample.

#### Performance Measures

* Q: list all performance measures to be estimated
* Q: talk about how relevant they are to the estimands?
* Q: choose value of n_sim which achieves acceptable Monte Carlo SE (?)

* A: we will assess the following two performance measures for each method...

* A: we will calculate the mean squared error (MSE) in order to assess the quality of each of our estimators ($\bar{X}$ and $s$). the MSE measures the average squared differences btwn our estimators and their respective parameters, which essentially means it measures how far off our estimators are from the parameters.