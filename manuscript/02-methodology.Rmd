---
output:
  pdf_document: default
  html_document: default
---

```{r load_packages2, include = FALSE}
library(mosaic)
library(knitr)
```

# Methodology {#methodology}

## Overview {#overview}

* There are many different ways that researchers are dealing with detection limits, which are ubiquitous in the scientific realm. Substitution, nonparametric methods, and maximum likelihood methods are all ways to combat this problem. [@Lafleur2011]

* Detection limits are constantly changing: as technology improves, so too does our ability to accurately measure substances [@Elias1999]

* Reporting limits can often be used be misleading as non-statisticians (practices in environmental law) may often interpret ND (non-detect) as nullity, when in fact it only means that the measurement falls below a certain limit [@Elias1999]

* Substitution is the worst way, nonparametric ways do better, maximum likelihood methods are the best [@Lafleur2011]

* The general form of the equation used to determine LOD is of the form:
  $$LOD = k * s_{zero}$$
  where "k is the constant for defining LOD" and $s_{zero}$ is the standard deviation of the zero/blank" [@Akard2002]
  
* 2 or 3 SDs is often what is used, refer to picture (I think it's neat) [@Akard2002]

* Throwing away/Discarding values that are below of the LOD gets rid of tons of useful information [@Berthouex2020]

*  LOD entries still contain information that a lot of people don't realize -- specifically information that the values is between 0 and the LOD. [@Chen2011]

* The way that chemists/researchers report low concentrations are varied and not standardized. Some may report: "ported. They may report the datum to the data analyst as ( 1 ) trace, ( 2 ) the letters ND ( not detected ), ( 3 ) the numerical value of MDL itself, (4) a "less than" value, that is, the numerical value of the MDL preceded by a "<" sign, (5) zero, (6) some value between zero and the MDL, for example, one-half the MDL, (7) the actual measured concentration even if it is below the MDL ( that is, whether the value is positive or negative ), ( 8 ) the actual measured value followed by the MDL in parenthesis, or (9) the actual measured value with a statement of its precision (for example, $2  \pm 4$ Mg/L, where the $\pm$ value indicates the precision of the estimate ). The last three methods are the best." [@Berthouex2020]

### Substitution Approach {#Substitution}

* Substitution methods are easy to implement, but are biased (common values: LOD/2, LOD/sqrt(2), LOD) but are discouraged b/c results in estimates of parameters being biased [@Chen2011]

* First method they tried was replacing all values with LOD/2, they claim these method was recommended for datasets where lots of data is below LOD or when data is highly skewed with a geometric sd of 3 or more. Some people also use LOD/sqrt(2) and is recommended to be used when few data us below LOD or when data is not highly skewed  Through their study, they found that there really isn't a difference between these two methods [@Glass2001]

### Kaplan-Meier Estimate Approach (#Kaplan-Meier)

* Suggests using the reverse Kaplan-Meier (KM) estimator to estimate the distribution function and population percentiles for data where there is "left-censored data" (data point is below a certain value but known by how much) [@Gillespie2010]

* After their study, they found that even though THEORETICALLY the reverse KM is for left-censored data (just like KM is for right-censored data), it is still limited in its usage since all it does it estimate a distribution function (it's really just an exploratory thing, they said) [@Gillespie2010]

### Distribution-Based Multiple Imputation Approach (#DBML)

* Study exploring different options to handle LOD laboratory data -- specifically with regards to multiple imputation methods for left-censored data. They concluded that "the distribution-based MI method" worked well for bivariate data where the values were < LOD. [@Chen2011]

* What this study used was distribution-based multiple imputation methods -- they used MLEs to estimate distribution parameters based on all datas (< LOD and those not). They repeatedly imput the values to create multiple complete sets of data, and then analyzed each one individually [@Chen2011]

* Mathematically, they created a log-likelihood function with all the data, then derived MLEs of each parameters on multiple bootstrapped datasets. Each bootstrap data gives different estimates for the mean, sd, etc. (refer to article for math) [@Chen2011]

### Write about other approaches here

* Another method is "Cohen's Method" where one extrapolates the left hand side of distribution based on the distribution of the uncensored data and then calculate the MLE estimate of the arithmetic mean -- found to be unreliable with data with outliers, this method can ONLY be used with data where there is a single LOD. From their study they found that this method gave high, unlikely results of the mean [@Glass2001]

* Imputing from a uniform is useful when you don't know the distribution of the data [@Canales2018]

### Comparison between different methods

* Compared 5 methods: found that in terms of performance:  imputation method using MLE to estimate distribution parameters and then imputing censored data points with values from this distribution below the LOD >  imputation from a uniform distribution > other 3 methods (substitution, log-normal MLE to estimate mean and SD, and kaplan-meier estimate) [@Canales2018]

* KM method is better than MLE for data where there are TONS of missing data or if data is highly skewed (distribution not assumed in KM method) [@Canales2018]

* Uses RMSE (root mean squared error) to see how close the estimated values are to the true values (lower RMSE means closer estimation to known values) [@Canales2018]

### Packages

* MLE and KM methods were implemented using the NADA package (https://cran.r-project.org/web/packages/NADA/NADA.pdf) in R (`cenmle` and `cenfit` functions) where data is labeled as censored or uncensored, for censored values, LOD is used as a placeholder, since these methods aren't imputation methods -- these censored values weren't replaced. Instead, summary statistics were generated with the entire data set (including the censored data) [@Canales2018]

* The imputations methods used mostly followed the general ideas: we have to assume the entire data set follows a particular distribution. Then we use this distribution to impute in values for the censored data. The MLE imputation method uses MLE methods to estimate the parameters of a distribution to fit the dataset, then values lower than the LOD are imputed FROM this dataset for all censored values (they used the function `fitdistcens` from the R package `fitdistRplus`). The second uniform imputation method assumes a uniform distribution with minimum 0, maximum LOD -- for all values less than the LOD, then the left-censored values are replaced with a number randomly selected from this uniform distribution. [@Canales2018]