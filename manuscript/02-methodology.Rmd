---
output:
  pdf_document: default
  html_document: default
---

```{r load_packages2, include = FALSE}
library(mosaic)
library(knitr)
```

# Methodology {#methodology}

The concept of missing data is ubiquitous within academic disciplines and which frequently complicates any types of real-world studies. Missing data will be defined within this thesis as _occurences within a dataset where there is no value stored for a variable in the observation of interest_. As most studies often utilize data collected through mediums such as surveys, questionnaires, or field research, missing data is an unavoidable problem.  Missing data hinders one's ability to work with and analyze the phenomena at hand, as data is often the basis of all studies.  One of the most glaring issues with missingness is how it can introduce bias within an analysis, which can more often than not, invalidate a study if not accounted for and handled properly. 

This thesis will go into a more specific instance of missing data known as censoring, which is _the condition when one has only partial information regarding the values of a measurement within a dataset_. We will introduce and define the three types of censored data, challenges with the reporting of censored data, alongside a discussion on common statistical approaches to handling censored data. 

## Censored Data {#censored_data}

As discussed previously, censored data is a specific type of missingness where one has only partial information regarding the values of a measurement in a dataset. There are many types of censoring which can occur, but three main ones which are the most common: right censoring, interval censoring, and left censoring.

### Right Censoring {#right}

Right censoring is a specific instance in which we only know that the true value of a data point lies above a certain threshold, but it is unknown by how much. Suppose a study on income and mortality is conducted with the variable of interest, $T$, being the time measured from the start of the study to the death of the participant. The study has a duration of 5 years, in which participants are expected to submit a form regarding their annual income. The value for the participant would be considered to be right-censored if at any point during the study, they failed to follow-up, or if the participant was still alive at the conclusion of the 10 year study. In this design study, several possibilities can occur, illustrated in figure blah.

```{r, echo = FALSE, rightcensoringexample, fig.cap="Right Censoring Example", out.width = '100%'}
knitr::include_graphics(path = "figures/right_censoring_example.png")
```

If the individual passes to the termination of the study, the only information we have is $T > 5$. 

If an individual passes away at some point, $t_i$ during the study, then $T = t_i$.

Suppose an individual stopped submitting a form during the third year of the study. As we have no information about whether or not if they died or simply did not submit their form, all we know is that the individual died/will die at some point after year three of the study. In this instance, $T > 3$.

Right censoring is the most common type of censoring and can often be found in clinical trial studies, mortality studies, and other forms of surival analyses.

### Left Censoring {#left}

```{r, echo = FALSE, leftcensoringexample, fig.cap="Left Censoring Example", out.width = '100%'}
knitr::include_graphics(path = "figures/left_censoring_example_fix.png")
```

In contrast with right censoring, left censoring is a specific instance of censoring in which we only know that the true value of a data point falls below a certain threshold which we call the _limit of detection_ (LOD). 

To understand this concept better, consider the following example. Imagine a scenario in which you are attempting to estimate the time at which the sun rises each morning. You plan to wake up every morning far before the sun rises, but on the first day of the study, you oversleep and wake up at 7:00 A.M. with the sun already out. We now have an instance of left-censored data. We want to know the time at which the sun rose, but all we have is an upper limit (7:00 A.M.). 

Left censoring is commonly found in environmental, water quality, and chemical-related research, where the focus is on the concentration of an analyte. Due to limitations on measuring instruments, left censored data are commonly found in these types of studies.

### Interval Censoring {#interval}

```{r, echo = FALSE, intervalcensoringexample, fig.cap="Interval Censoring Example", out.width = '100%'}
knitr::include_graphics(path = "figures/interval_censoring_example_fix.png")
```

Interval censoring is another form of censoring in which the random variable of interest is known to be between an interval of two values. Considering a random variable $T$, which denotes the survival time of interest, if interval censoring is at hand, we can denote the interval containing $T$ to be $I = [L, R]$, with $L$ being the beginning of the interval and $R$ being the end of the interval.

Left and right censoring are special cases of interval censoring. In the case of left censoring, $L = 0$; and conversely in the case of right censoring, $R = \infty$.

To conceptualize interval censoring, we can consider a example study on virus testing, in which participants get their blood drawn in order to detect whether or not if they test positive for the virus or not. The random variable in question is $T$, which represents the exact timepoint at which the subject contracted the virus. If an individual was first tested at time $t_1$ and tested negative, but was tested again at a later time $t_2$ and tested positive, the specific time $t$ at which the subject contracted the virus is unknown. All we know is that it lies somewhere between the interval is $I = [t_1, t_2]$, but not the exact time at which they contracted it.

There are a myriad of types of censoring which can be discussed, however the focus of my thesis deals specifically with the challenges of reporting these values alongside methods of handling left-censored data.

## Challenges of Reporting Censored Data {#challenges}

[CURRENTLY WORKING ON REORGANIZING HERE]

There is no universal reporting practice for values below the LOD which can lead to confusion amongst researchers. The lack of standardization makes it difficult to distinguish LOD values with uncensored values. This can lead to LOD values unintentionally being overlooked, causing faulty analysis or conclusions which are heavily flawed.

In a study involving the precision of lead measurements near concentrations of the limit of detection, Berthouex (1993), discusses this disparity in practice within chemists and their labs and describe in the following list as:

\begin{enumerate}
  \item Reporting the trace, a chemical whose average concentration is less than 100 $\mu g$
  \item Reporting  the letters ND, which stand for "not detected"
  \item Reporting the numerical LOD value itself
  \item Reporting the "less than" value, which is the numerical LOD preceded by a "<"
  \item Reporting some value between 0 and the LOD, such as one-half the LOD value
  \item Reporting the actual measured concentration, even if it falls below the LOD
  \item Reporting the actual measured concentration, followed by (LOD)
  \item Reporting the actual measured concentration with a precision ($\pm$) statement
\end{enumerate}

The latter three methods are the best [FIND PAPER BY Gilbert, 1987; Hunt and Wilson, 1986; and Rhodes, 1981 -- backtrace through berthouex when time allows? by what metrics?].

<!-- In an article published by the American Bar Association, a company seeking to purchase land for residential use, hires several laboratories to conduct analyses of the chemical concentrations at that site. However, miscommunication occurs between the two parties, in which the laboratory mistakenly is given information that the site is used for industrial purposes. As it turns out, detection limits for residential areas differs from industrial ones, and as a result, the analytic results turned out useless for the company. This situation is a clear example of how [@Elias1999]. -->

Berthouex discusses the prevalence in regards to the practice of censoring data by reporting only values which are above the detection limit and discarding those which fail to yield quantifiable results. He discourages this practice and instead suggests the reporting the numeric values of measurements, even when those values are below the limit of detection. [@Berthouex1993]

## Approaches {#Approaches}

It is important to note that the values below the LOD still contain information, specifically that the values is between the lower bound value (if it exists) and the LOD [@Chen2011]. As such, there are a variety of statistical treatments to handle censored data which have been popularized in the statistical literature which will be discussed within this section.

Omission involves the deletion of data points which are deemed to be invalid, as a result of left-censoring or any other deficiencies in the data. This is also more commonly known as *available-case analysis*, in which statistical analysis is conducted while only considering the observations which have no missing data on the variables of interest, and excluding the observations with missing values [@May2012]. May argues against this approach and claims that the loss of information from discarding data and the inflation of standard errors of estimates (when discussing missingness in a regression context) will invariably be inflated as a result of the decreased sample size. The advantages of omission lies in its ease of implementation. [WHAT SORT OF USEFUL INFORMATION IS DISCARDED, INCLUDE HERE, omitting is a valid method which is used by default in a lot of statistical software, when should care be used?]

Apart from available-case analysis, over the past century, a myriad of methods to deal with censoring have been developed to counter this issue -- some more statistically sound than others. We will review some of the most common methods to estimate descriptive statistics involving censored data, which include: substitution, maximum likelihood estimation, Kaplan-Meier, and regression on order statistics [@Lafleur2011].

### Substitution Approach {#Substitution} 

Often condemned in papers, as a statistically unsound method to handle censored data, substitution methods are ubiquitous in the chemical and environmental sciences as an appropriate and recommended method to work with left-censored chemical concentration data [@Canales2018].

The substitution method simply involves imputing in a replacement value in lieu of the censored data point. The lack of a global, standardized replacement value to substitute is one of the most pronounced downside of this method. These replacement value used may differ between studies but common values include: $\frac{LOD}{2}, \frac{LOD}{\sqrt2}$, or $LOD$ [@Lee2005]. Different disciplines have their own suggested "best" replacement value to use, an example being $\frac{3}{4}$ times the LOD being a common replacement value in geochemistry [@Crovelli1993]. However, it must be recognized that the substitution method is a statistically *unsound* technique which are used often in non-rigorous statistical settings due to them being quite easy to implement [@Chen2011]. As such, there have seen several studies in order to investigate the effectiveness of the method. 

One particular investigation was conducted by Glass and Gray (2001) to investigate the effectiveness of LOD approaches, they used a variety of naive substitution methods from the values listed previously, with LOD/2 and LOD/√2. Proponents of the substitution method claim that the replacement value $\frac{LOD}{2}$ is useful for data sets in which the majority of the data are below the LOD or when the distribution of the data is highly skewed; the definition of "highly skewed" being any distribution with a geometric standard deviation (a measure of spread commonly used in tandem with log-normal distributions) of 3 or more [@Hornung1989]. They also suggest using $\frac{LOD}{\sqrt2}$ when there are only a few data points below the LOD or when the data is not highly skewed. 

Substitution methods are flawed as they can often introduce a "signal" which was not originally present within the data, or even obstruct an actual signal which was present in the original data [@Lee2005]. Numerous authors have advised against the usage of substitution methods, for being statistically inappropriate to use. Glass and Gray (2001) found that both introduce large errors and biases in descriptive statistics of interest. Thompson and Nelson (2001) conducted a study in which they found similar results, in that it often led to biased parameter estimates and "artificially small standard error estimates." Hewett and Ganser (2007) also found in their simulation study that the substitution method yielded the lowest average bias and root mean squared error values (comparison metrics to measure accuracy) in their estimation of the mean. Overall, the overall consensus seems to advise against the usage of these substitution techniques.

### Maximum Likelihood Estimation {#MLE}

Maximum likelihood estimation (MLE) is a parametric technique which allows us to estimate the parameters of a distribution or model when the data is from a multivariate normal distribution.

To give a brief introduction to the mechanisms of MLE and how it functions, given a random, independently and identically distributed (*i.i.d.*) set of random variables $X_1, X_2,...,X_n$ from distribution $f(x|\theta)$.

For every observed random sample $x_1,...,x_n$, we can define the joint density function to be:

$$f(x_1,...,x_n|\theta) = f(x_1|\theta)...f(x_n|\theta) = \prod_{i=1}^{n}f(x_i|\theta)$$
Upon observing the given data, $f(x_1,...,x_n|\Theta)$ becomes a function of $\theta$ alone, so we obtain a likelihood of:

$$lik(\theta) = f(x_1,...,x_n|\theta)$$
Our goal is to obtain the maximum likelihood estimate of $\Theta$ which maximizes $lik(\theta)$, in other words, to obtain a $\theta$ which makes our observed data the most probable/likely.

As we previously declared our random variables $X_1, X_2,...,X_n$ to be i.i.d, we can rewrite the likelihood to be a product of the marginal densities:

$$lik(\theta) = \prod_{i=1}^{n} f(x_i|\theta)$$
in which we can then maximize the likelihood to find the best mle of $\theta$ to best capture our observed data.

Maximum likelihood estimation is widely thought to be optimal, but only if one knows the proposed model and underlying distribution of the dataset in advance, hence its classification as a parametric technique. In a study comparing methods to handling missing data, Canales found that the MLE method underperformed when the data in question was highly skewed, in which overinflated mean squared errors were often obtained [@Canales2018].

The MLE method we will be utilizing is actually performed by obtaining regression estimates of slope(s) and intercepts through maximum likelihood with censored data. The `cenmle` function in the `NADA` package allows the user to specify censored and uncensored data, and uses the LOD as the placeholder. As this method is not an imputation technique, values are not replaced. This method allows us to calculate the summary statistics for the entire data set -- including the censored values.

* Useful slides to refer to here [https://www.eurachem.org/images/stories/workshops/2017_10_PT/pdf/contrib/O05-Mancin.pdf]

### Kaplan-Meier Estimate Approach {#Kaplan-Meier}

[As a phenomenon, censoring is most often discussed in the branch of statistics known as survival analysis, which concerns itself with techniques to analyze a time to an event variable. As their name suggests, these variables measure the time which passes until some sort of event occurs. The type of event being observed need not be related to issues related to mortality, but it is certainly is most commonly employed in the health-care field. These types of events can be as innocuous as the time until device breaks, time until birds migrate away from their homes, or even things like time until an ice cream scoop falls onto the pavement. Regardless of which, all of these scenarios share a common flaw in terms of the possibility of the data being "censored."] [need to fix bracketed into existing paragraph]

The Kaplan-Meier method is a common nonparametric technique used to deal with censored data. Originally developed to handle right-censored survival analysis data, an offshoot method in the form of the Reverse Kaplan-Meier Estimator have sprung up as a way to handle left censored data as well [@Gillespie2010]. The advantages of the KM method lie in its robustness as a nonparametric method; it performs well with a wide range of distributions. Many recommend its usage for when there are cases of extreme/severe censoring as a result of this [Canales2018].

To introduce the concept of the KM-estimator, it is helpful to take a look into its usages in survival analysis studies where the focus is often on a type of "time to a certain event occurring", often being cases such time to death, or time to failure.

* [INSERT PICTURE OF EXAMPLE SURVIVAL CURVE HERE]

The KM-estimator is a nonparametric statistic used to estimate the survival curve from the empirical data while accounting for the possibilities of certain values being censored (participants in a mortality study could drop out, die during the study, become unavailable to contact after a certain time, etc.). It does this by assuming that censoring is independent from the event of interest (death) and that survival probabilities remain the same in observations found early in the study and those recruited later in the study [CITE PROPERLY https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/BS704_Survival_print.html]

The KM-estimator when performing an empirical estimation of the survival curve at time $t$ can be represented by the following equation:

$$\hat{S}(t) = \prod_{\ x_j \le \ t }(1-\frac{d_j}{y_j})$$
where $x_j$ is the distinct event/death time, $d_j$ is the number of event/death occurrences at time $x_j$, and $y_j$ is the number of followup times ($t_i$) that are $\ge$ $x_j$ (how many observations in sample survived at least/or past the time $t_i$). [CITE PROPERLY WHEN TIME ALLOWS https://www.youtube.com/watch?v=NDgn72ynHcM&t=398s&ab_channel=mathetal]

Typically, the KM-estimator can only be used to estimate the distribution function of right-censored data, in which a data point is above a certain threshold, but it is unknown by how much. A simple tweak to the typical KM-method yields the reverse Kaplan-Meier approach, which allows for the estimation of the survival curve with left-censored values. This approach follows exactly the same logic as the Kaplan-Meier estimate of the survival curve, except we reverse the censored indicator and event of interest indicator. In other words, our censor is now the event and the event is now censored. This allows us to estimate the distribution function and population percentiles for data containing left-censored values [@Gillespie2010].

For our analysis, we will be using the `cenfit` function from the `NADA` package in R to estimate the empirical cumulative distribution function (survival curve) for our left-censored data using the reverse Kaplan-Meier method. Similarly to the MLE method, the KM method is not an imputation method, so we are not replacing censored values with an imputed value, but rather estimating descriptive statistics for the entire dataset -- including the censored concentrations [@Canales2018].

### Regression on Order Statistics {#ROS}

In between both the parametric nature of the MLE approach and nonparametric of the Kaplan-Meier estimator is the regression on order statistics (also known as ROS) method. As its name suggests, ROS is a semi-parametric method. It assumes that the censored measurements (emphasis on ONLY the censored, this what makes it semi-parametric) in the data comes from a normal or lognormal distribution. 

In the ROS method, in order to model the distribution of the censored values, a linear regression model is created by plotting the uncensored observed values (ordered from smallest to largest) vs. the quantiles (also known as "order statistics"), which is then used estimate and impute the values of the censored data [@Lee2005]. These imputed values for the censored portions of the data are then combined with the known values of the uncensored bits, which allows for the computation of the descriptive statistics of interest. In summary, ROS imputes the censored data using the estimated parameters from the linear regression model of the uncensored observed values versus their quantiles.

There are of course, some requirements which must hold in order for ROS to be utilized: at minimum, there needs to be at least 3 known values and more than half the values within the data set must be known. As regression is utilized in this method, additional assumptions in the ROS method are shared with those necessary for linear regression to be performed as well. The response variable must be a linear function of the explanatory variable (quantiles). Additionally, the errors should have constant variance [@Lee2005].

The `NADA` package contains the function `ros` which provides an implementation of regression on order statistics which allows us to calculate descriptive statistics for left censored values.