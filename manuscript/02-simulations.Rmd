---
output:
  pdf_document: default
  html_document: default
---

# Simulations {#simulations}

Having discussed four common methods to handle left-censored data in the previous chapter, we now turn to a simulation study in order to evaluate the strengths and weaknesses of each method after varying the censoring rate, sample size, and the underlying distribution of the data. We will also discuss the implementation of the methods, data generating mechanisms for the simulation study, and our findings.

## Data-Generating Mechanisms {#data_generating_mechanisms}

To reflect the typical distributions of left-censored data, we generated data for use in our simulation study with parametric draws from the log-normal, exponential, and Weibull distribution. We also adjusted sample sizes, $n = \{10, 100, 1000\}$ and censoring rate, $R = \{0.10, 0.30, 0.50\}$. The sample sizes are chosen as such to reflect realistic sizes for water quality datasets, which we plan to investigate in the following chapter, with a case-study on coal ash contamination in water wells. Regarding the values we chose for $R$, if we recall from the previous chapter, the ROS method is unable to be implemented when more than half the data is censored, which is why we chose our censoring rates as such. 

In the following sections of this chapter, we will interchangeably use "small, medium, large" and "low, medium, and high" to differentiate between the different values for sample size and censoring rate, respectively. For example, "small sample size" is equivalent to $n = 10$ and "high censoring rate" is equivalent to $R = {0.50}$.

To artificially induce censoring in our simulated data, we arranged the uncensored observations in ascending order and set those in the lowest $100R \%$ to be censored, and the remaining observations as uncensored. As an example, if the censoring rate was $R = 0.10$, the lowest 10% of the observations would be marked as censored while the rest remained uncensored.

## Estimands {#estimands}

Each of the four methods discussed in the previous chapter are designed for usage in obtaining summary statistics for left censored data [@Shoari2018]. In preparation for our case study in the following chapter, we will be utilizing the four methods to estimate the mean of the censored variable of interest in our simulation study.

## Implementation in R {#implementation}

All our code will be written using R. For our substitution method, we begin by defining the LOD as the minimum, uncensored value for the variable of interest. We will choose LOD/2 as our replacement value, calculating our estimated mean using this newly defined dataset.

For the remaining three methods (KM, MLE, and ROS), we will be using specialized functions from the `NADA` package. The code for the MLE method will be handled with the `cenmle` function in the `NADA` package, which allows the user to specify censored and uncensored data, and uses the LOD as the placeholder. As this method is not an imputation technique, values are not replaced. This method allows us to calculate the summary statistics for the entire data set -- including the censored values. The `cenfit` function allows us to implement KM. This function "computes an estimate of an empirical cumulative distribution function (ECDF) for censored data using the Kaplan-Meier method," from which we can then obtain summary statistics of interest. Similarly, the `ros` method implements ROS and outputs a dataframe, from which we can then use to also compute our summary statistics of interest.

## Performance Measures {#performance_measures}

The criterions we will use to assess the performance of each of our four methods will consist of: bias, variance, and mean squared error (MSE), which measure the difference between our mean estimate and the true mean, precision, and accuracy -- respectively. 

<!--

### Variance

Prior to defining variance, we must discuss the concept of _precision_. Precision simply refers to how far away estimates from different samples are from one another. Low precision indicates that the estimates from each sample are far from one another in value, while high precision indicates the opposite. Knowing this, _variance_ is a metric which informs us on the precision of an estimator. It is defined as the average squared deviation of the estimator from its average:

$$Variance = E[(\hat{\mu}-E(\hat{\mu}))^2]$$

Estimators with low variances generally remain close in value throughout all samples, while those with high variance may wildly differ between samples. It is generally preferable to have an estimator with low variance. Precision measurements, such as the variance, are not a sole indicator of an estimator's performance [@Walther2005]. While useful for assessing how close values are to one another, it is just as important to obtain the estimator's $bias$, a measure of how close the obtained estimate is to the true value.

### Bias

The next performance metric which we will use is bias, which is defined as the difference between an estimator's expected value and the true value of the parameter. In our case, we are using the estimator $\hat{\mu}$ to estimate the true population mean, $\mu$, in each of our samples. The formal definition of bias is as follows: 

$$Bias = E(\hat{\mu}) - \mu$$

Bias informs us on the difference of the estimator from the true parameter. A natural question to ask is often whether if an estimator is any "good". One possible measures of this idea "good," naturally comes in the idea of an unbiased estimator. If the bias of an estimator were to be equal to zero, we would define the estimator to be _unbiased_, meaning that the estimator produces parameter estimates which are on average, equal to the true value. 

An estimator being unbiased does not necessarily equate to it being ideal. An unbiased estimator could have high variance, which would mean that the estimator in each sample would be significantly different from one another, but on average -- they equal the true population estimand. 

On the opposite hand, it would also not be very useful if an estimator had low variance but high bias. This would mean that each sample would consistently produce similar estimates which are very far from the true population estimand.

### Mean Squared Error (MSE)

Evidently we do not want our estimator to be too biased nor too variant. This conflict is known as the _bias-variance tradeoff_, a dilemma in which we can never simultaneously minimize the bias and variance of our estimator.

While we generally would like estimators which have low bias and low variance, it can be difficult to achieve both at once. As such, it is common to instead turn to a quantity known as the _mean squared error_ (MSE), which is a quantitative measurement used to assess the accuracy of an estimator. The MSE measures how far away, on average, an estimator is from its true value and makes use of both bias and variance in its calculations. The formal definition of MSE is:

$$MSE = E[(\hat{\mu} -\mu)^2] = Var(\hat{\mu})+[Bias(\hat{\mu})]^2$$

We can show that the MSE of estimator can be rewritten in terms of its variance and bias:

$$E[(\hat{\mu} -\mu)^2] = E(\hat{\mu}^2) + \mu^2 - 2E(\hat{\mu})\mu$$
From $Bias = E(\hat{\mu}) - \mu$, it follows that:

$$Bias^2 = E^2(\hat{\mu}) +\mu^2 -2E(\hat{\mu})\theta$$

From $Variance = E[(\hat{\mu}-E(\hat{\mu}))^2] = E(\hat{\mu}^2) - E^2(\hat{\mu})$, we can combine the square of the bias with variance, which yields:

$$Bias^2 + Var = [E^2(\hat{\mu}) +\mu^2 -2E(\hat{\mu})\theta] + [E(\hat{\mu}^2) - E^2(\hat{\mu})]$$

The $E^2(\hat{\mu})$ terms cancel out, and we are left with:

$$E^2(\hat{\mu}) +\mu^2 -2E(\hat{\mu})\theta = E[(\hat{\mu} -\mu)^2] = Bias$$

As the MSE is always positive, MSE values closer to zero are more desirable -- as it is an indicator that the estimator is accurate.

-->

## Results {#results}

```{r libraries, include = FALSE}
library(tidyverse)
library(Metrics) #package to help calculate mse
library(NADA) #package with implementation of many methods
library(survival)
library(kableExtra)
```

```{r generateLN, include = FALSE}
#function will generate a vector of numbers from the log-normal 
#distribution and censor them at the given rate
#function will take in arguments for 1) samplesize, 2) logmean, 3)logsd
#4) censoring rate

generateLN <- function(sampsize, m, s, censrate){
  true.value <- rlnorm(sampsize, 
               meanlog=log(m^2 / sqrt(s^2 + m^2)),
               sdlog=sqrt(log(1 + (s^2 / m^2))))
  
  uncensored_df <- as.data.frame(true.value) %>%
    arrange(true.value)
  
  censored_df <- uncensored_df %>% #take the head(%) of data to be censored
    slice_head(n=nrow(uncensored_df)*censrate) %>%
    mutate(censored = TRUE)
  
  #full join original df and sliced df
  return_df <- full_join(uncensored_df, censored_df, by = "true.value")

  #replace NAs with FALSE
  return_df$censored <- replace_na(return_df$censored, replace = FALSE)
  
  return(return_df)
}
```

```{r generateEXP, include = FALSE}
#function will generate a vector of numbers from the exponential 
#distribution and censor them at the given rate
#function will take in arguments for 1) samplesize, 2) rate,
#3) censoring rate

generateEXP <- function(sampsize, r, censrate){
  true.value <- rexp(sampsize, rate = r)
  
  uncensored_df <- as.data.frame(true.value) %>%
    arrange(true.value)
  
  censored_df <- uncensored_df %>% #take the head(%) of data to be censored
    slice_head(n=nrow(uncensored_df)*censrate) %>%
    mutate(censored = TRUE)
  
  #full join original df and sliced df
  return_df <- full_join(uncensored_df, censored_df, by = "true.value")

  #replace NAs with FALSE
  return_df$censored <- replace_na(return_df$censored, replace = FALSE)
  
  return(return_df)
}
```

```{r generateW, include = FALSE}
#function will generate a vector of numbers from the Weibull
#distribution and censor them at the given rate
#function will take in arguments for 1) samplesize, 2) rate,
#3) censoring rate

generateW <- function(sampsize, sh, sc, censrate){
  true.value <- rweibull(sampsize, shape = sh, scale = sc)
  
  uncensored_df <- as.data.frame(true.value) %>%
    arrange(true.value)
  
  censored_df <- uncensored_df %>% #take the head(%) of data to be censored
    slice_head(n=nrow(uncensored_df)*censrate) %>%
    mutate(censored = TRUE)
  
  #full join original df and sliced df
  return_df <- full_join(uncensored_df, censored_df, by = "true.value")

  #replace NAs with FALSE
  return_df$censored <- replace_na(return_df$censored, replace = FALSE)
  
  return(return_df)
}
```

```{r setup, include = FALSE}
iterations <- 1000 #number of iterations
censvalues <- c(0.10, 0.30, 0.50)
sampsizes <- c(10, 100, 1000)

df.tall <- data.frame(prop_cens = numeric(),
                      samplesize = numeric(),
                      iteration = numeric(),
                      method = character(),
                      true_mean = numeric(),
                      mean_complete = numeric(),
                      mean_method = numeric(),
                      true_sd = numeric(),
                      SE_complete = numeric(),
                      SE_method = numeric())
```

```{r log-normal, echo = FALSE, message = FALSE, cache = TRUE}
#log-normal
options(scipen=999) #prevent scientific notation
set.seed(7271999)

for(i in censvalues){
  for(j in sampsizes){
    for(k in 1:iterations){
      m <- 1
      s <- 0.5
      df <- generateLN(sampsize = j, m = 1, s = 0.5, censrate = i)
      
      #substitution
      #define LOD to be smallest, uncensored value
      LOD <- min(df$true.value[df$censored == FALSE]) 
      df <- df %>%
        mutate(impSubValue = if_else(censored == TRUE, LOD/2, true.value))
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "substitution",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(df$impSubValue),
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(df$impSubValue)/sqrt((length(df$impSubValue))))
      
      #mle
      mle_res = cenmle(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "mle",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(mle_res)[1],
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(mle_res)[2])
      
      #km
      km_res = cenfit(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "km",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(km_res)[[1]],
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(km_res)[[2]])
      
      #ros
      ros_res = ros(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "ros",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(ros_res),
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(ros_res)/sqrt((length(df$true.value))))
    }
    #end of # iterations
  }
}

#aggregating performance criteria

df.ln <- df.tall %>%
  group_by(prop_cens, samplesize, method) %>%
  summarize(Avg_Mean = mean(mean_method),
            Bias = (mean(mean_method) - true_mean),
            Variance = var(mean_method),
            MSE = mse(true_mean, mean_method) 
            ) %>%
  distinct() %>%
  ungroup()
```

```{r exponential, echo = FALSE, message = FALSE, cache = TRUE}
#EXPONENTIAL
options(scipen=999) #prevent scientific notation
set.seed(7271999)

for(i in censvalues){
  for(j in sampsizes){
    for(k in 1:iterations){
      r = 1
      df <- generateEXP(sampsize = j, r = r, censrate = i)

      #substitution
      #define LOD to be smallest, uncensored value
      LOD <- min(df$true.value[df$censored == FALSE]) 
      df <- df %>%
        mutate(impSubValue = if_else(censored == TRUE, LOD/2, true.value))
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "substitution",
                true_mean = 1/r,
                mean_complete = mean(df$true.value),
                mean_method = mean(df$impSubValue),
                true_sd = 1/r,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(df$impSubValue)/sqrt((length(df$impSubValue))))
      
      #mle
      # mle_res = cenmle(df$true.value, df$censored)
      # 
      # df.tall <- df.tall %>%
      #   add_row(prop_cens = i,
      #           samplesize = j,
      #           iteration = k,
      #           method = "mle",
      #           true_mean = 1/r,
      #           mean_complete = mean(df$true.value),
      #           mean_method = mean(mle_res)[1],
      #           true_sd = 1/r,
      #           SE_complete = 
      #             sd(df$true.value)/sqrt((length(df$true.value))),
      #           SE_method = mean(mle_res)[2])
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "mle",
                true_mean = NA,
                mean_complete = NA,
                mean_method = NA,
                true_sd = NA,
                SE_complete = NA,
                SE_method = NA)
      
      #km
      km_res = cenfit(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "km",
                true_mean = 1/r,
                mean_complete = mean(df$true.value),
                mean_method = mean(km_res)[[1]],
                true_sd = 1/r,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(km_res)[[2]])
      
      #ros
      ros_res = ros(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "ros",
                true_mean = 1/r,
                mean_complete = mean(df$true.value),
                mean_method = mean(ros_res),
                true_sd = 1/r,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(ros_res)/sqrt((length(df$true.value))))
    }
    #end of # iterations
  }
}

#aggregating performance criteria

df.exp <- df.tall %>%
  group_by(prop_cens, samplesize, method) %>%
  summarize(Avg_Mean = mean(mean_method),
            Bias = (mean(mean_method) - true_mean),
            Variance = var(mean_method),
            MSE = mse(true_mean, mean_method) 
            ) %>%
  distinct() %>%
  ungroup()
```

```{r weibull, echo = FALSE, message = FALSE, cache = TRUE}
#WEIBULL
options(scipen=999) #prevent scientific notation
set.seed(7271999)

for(i in censvalues){
  for(j in sampsizes){
    for(k in 1:iterations){
      sh = 1
      sc = 1
      df <- generateW(sampsize = j, sh = sh, sc = sc, censrate = i)

      #substitution
      #define LOD to be smallest, uncensored value
      LOD <- min(df$true.value[df$censored == FALSE]) 
      df <- df %>%
        mutate(impSubValue = if_else(censored == TRUE, LOD/2, true.value))
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "substitution",
                true_mean = sc*gamma(1+(1/sh)),
                mean_complete = mean(df$true.value),
                mean_method = mean(df$impSubValue),
                true_sd = sqrt((sc^2)*(gamma(1+(2/sh)) - 
                                      (gamma(1+(1/sh)))^2)),
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(df$impSubValue)/sqrt((length(df$impSubValue))))
      
      #mle
      # mle_res = cenmle(df$true.value, df$censored)
      # 
      # df.tall <- df.tall %>%
      #   add_row(prop_cens = i,
      #           samplesize = j,
      #           iteration = k,
      #           method = "mle",
      #           true_mean = 1/r,
      #           mean_complete = mean(df$true.value),
      #           mean_method = mean(mle_res)[1],
      #           true_sd = 1/r,
      #           SE_complete = 
      #             sd(df$true.value)/sqrt((length(df$true.value))),
      #           SE_method = mean(mle_res)[2])
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "mle",
                true_mean = NA,
                mean_complete = NA,
                mean_method = NA,
                true_sd = NA,
                SE_complete = NA,
                SE_method = NA)
      
      #km
      km_res = cenfit(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "km",
                true_mean = sc*gamma(1+(1/sh)),
                mean_complete = mean(df$true.value),
                mean_method = mean(km_res)[[1]],
                true_sd = sqrt((sc^2)*(gamma(1+(2/sh)) - 
                                      (gamma(1+(1/sh)))^2)),
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(km_res)[[2]])
      
      #ros
      ros_res = ros(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "ros",
                true_mean = sc*gamma(1+(1/sh)),
                mean_complete = mean(df$true.value),
                mean_method = mean(ros_res),
                true_sd = sqrt((sc^2)*(gamma(1+(2/sh)) - 
                                      (gamma(1+(1/sh)))^2)),
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(ros_res)/sqrt((length(df$true.value))))
    }
    #end of # iterations
  }
}

#aggregating performance criteria

df.w <- df.tall %>%
  group_by(prop_cens, samplesize, method) %>%
  summarize(Avg_Mean = mean(mean_method),
            Bias = (mean(mean_method) - true_mean),
            Variance = var(mean_method),
            MSE = mse(true_mean, mean_method) 
            ) %>%
  distinct() %>%
  ungroup()
```

```{r, echo = FALSE, message = FALSE, cache = TRUE}
ln.table <- df.ln %>%
  select(-"prop_cens") %>%
  select(c(2, 1, 3:6)) %>%
  rename(" " = "method", "Sample Size" = "samplesize", 
         "Avg. Mean" = "Avg_Mean")

exp.table <- df.exp %>%
  select(-"prop_cens") %>%
  select(c(2, 1, 3:6)) %>%
  rename(" " = "method", "Sample Size" = "samplesize", 
         "Avg. Mean" = "Avg_Mean")

w.table <- df.w %>%
  select(-"prop_cens") %>%
  select(c(2, 1, 3:6)) %>%
  rename(" " = "method", "Sample Size" = "samplesize", 
         "Avg. Mean" = "Avg_Mean")
```

The results of our simulation study are presented in Tables \@ref(tab:lntable), \@ref(tab:exptable), and \@ref(tab:wtable).

```{r, lntable, echo = FALSE}
knitr::kable(ln.table, caption = "Performance metrics of our our 
             4 methods with data derived from the log-normal 
             distribution with mean = 1 and SD = 0.5.", 
             digits = 5, booktabs = "T") %>%
  kable_styling(font_size = 11.5) %>%
  pack_rows("Censoring Rate = 0.1", 1, 12) %>%
  pack_rows("Censoring Rate = 0.3", 13, 24, latex_gap_space = "1em") %>%
  pack_rows("Censoring Rate = 0.5", 25, 36, latex_gap_space = "1em")
```

```{r, exptable, echo = FALSE}
knitr::kable(exp.table, caption = "Performance metrics of our our 3 
             methods (MLE method absent) with data derived from the 
             exponential distribution with a shape parameter = 1.", 
             digits = 5, booktabs = "T") %>%
  kable_styling(font_size = 11.5) %>%
  pack_rows("Censoring Rate = 0.1", 1, 12) %>%
  pack_rows("Censoring Rate = 0.3", 13, 24, latex_gap_space = "1em") %>%
  pack_rows("Censoring Rate = 0.5", 25, 36, latex_gap_space = "1em")
```

```{r, wtable, echo = FALSE}
knitr::kable(w.table, caption = "Performance metrics of our our 3 methods 
             (MLE method absent) with data derived from the Weibull 
             distribution with a shape parameter = 1 and 
             scale parameter = 1.", 
             digits = 5, booktabs = "T") %>%
  kable_styling(font_size = 11.5) %>%
  pack_rows("Censoring Rate = 0.1", 1, 12) %>%
  pack_rows("Censoring Rate = 0.3", 13, 24, latex_gap_space = "1em") %>%
  pack_rows("Censoring Rate = 0.5", 25, 36, latex_gap_space = "1em")
```

### Log-normal {#log-normalsimstudy}

Table \@ref(tab:lntable) shows our simulation results for the log-normal distribution. In the case of low censoring (0.10), when dealing with sample sizes of 10 and 100, the methods are largely comparable to one another. Substitution and KM do not perform quite as well as ROS and MLE, both displaying an increase in absolute bias and MSE when compared to the latter two. Substitution performs significantly worse than KM, while MLE and ROS perform rather equally well in all sample sizes for low-censoring. 

When considering medium censoring (0.30), much of the same observations still hold true. Substitution performs the worse, followed by KM. MLE and ROS both perform well. However, ROS has a slight edge over MLE, especially as sample sizes increase, attaining lower MSE values than the latter.

In the case of the log-normally distributed data, all four methods begin to perform worse when the censoring rate is increased to 0.5, which is to be expected. As more and more missingness is introduced within the dataset, it becomes more difficult to obtain accurate estimates for all methods. Once again, substitution and KM attain high absolute bias and MSE values. However, it is now KM which performs worse than substitution in the setting of high censoring. Similarly to before, albeit being more noticeable now, ROS performs better than MLE with all sample sizes.

### Exponential and Weibull {#exponentialweibullsimstudy}

The results of the simulation study for the exponential and Weibull-distributed data can be viewed in Tables \@ref(tab:exptable) and \@ref(tab:wtable). We can see for these distribution, all three methods perform equally well in the case of low (0.10) censoring with all sample sizes, obtaining similar bias and MSE values across all sample sizes for both the exponential and Weibull datasets. KM consistently performs the worst with with medium (0.30) and high (0.50) censoring when compared to the other methods across all sample sizes. In these censoring settings, it is also the case that ROS performs the best with substitution not far behind.

In summary, regardless of distributional assumptions all of the methods perform well when censoring is low, with very minute differences in performance metrics. KM does not perform well with log-normally distributed data with high censoring rates and struggles in the exponential and Weibull cases with medium and high censoring rates. While MLE was only used in the case of the log-normal data, it performs quite well, although not quite as well as ROS. ROS performed the best in the case of medium and high censoring across all 3 distributions.

## Discussion {#discussion}

It important to note that while there are a large number of papers which discuss the ideal method or strategy to handle left-censored data,these studies have a large number of differences in censoring rates, distribution used, methods used, and other aspects of design setups which make comparisons regarding the results obtained from the studies quite difficult. As such, descriptions of specifics regarding the study design in the following studies will be omitted as necessary.

Several results from our simulation studies agree with previous findings conducted from other investigators in the field. @Gilliom1986 claims that with the log-normal distribution, the ROS method was superior. This claim is furthered with our own results: we find that the ROS method is rather robust, even with censoring and produces an accurate and precise estimate of the mean in all cases in our simulation study. 

Another investigation by @Kroll1996 found that with regards to a log-normal distribution only, ROS and MLE worked extremely well, with MLE outperforming the other methods especially in highly censored cases. While the MLE method did perform rather well in most cases in our simulation study, it did not outperform the ROS method, which in fact obtained much better estimates of the mean in highly censored settings. 

There are of course also studies which offer differing results from the ones we obtained in our simulation study.

@Schmoyer1996 compared only MLE and KM and found that the KM method performed nearly as well as the MLE in the case of the log-normal distribution. However, this was not the case in our simulation study. While the KM and MLE methods were able to perform adequately in the case of low (0.1) and medium (0.3) censoring, they performed the worst out of all four methods when dealing with highly censored cases. The censoring rates used in their study consisted of 25%, 50%, and 70% -- which far exceeded the censoring values used in this thesis. 

@She1997 conducted a study investigating censored water quality data with the intent of investigating how well the KM method performs with regards to the same methods we utilize in this thesis. The results from She's study showed that KM outperformed all other methods, which contradicts the findings in the simulation study conducted in this thesis. Upon further investigation, the size of the dataset utilized by She consisted of 56 observations from water monitoring stations, in which around 11 were censored (20% of the dataset). While the KM method is not ideal for highly censored cases from the results of our simulation study, it certainly is able to be used for smaller sample sizes -- which is further supported with She's findings. There may be a difference in how well the methods perform with actual data as compared to simulated data -- which we will investigate in the next chapter. 

### Limitations {#limitations}

Shortcomings in the results presented in this study may come from the fact that we generated data with known distributional parameters. It could be the case that the effectiveness of our methods were only due to having such artificial data. Alterations in our study to instead generate data from methods such as randomized pulls from an a real-world dataset of interest via. methods such as bootstrapping could provide different insights. As we discussed previously with the results from @She1997, methods may perform differently when utilized with artificial datasets as compared to real world, left censored data. We do not claim our findings in the simulation study to be representative for all cases of left censored data.
