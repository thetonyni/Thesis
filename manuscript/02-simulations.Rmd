---
output:
  pdf_document: default
  html_document: default
---

# Simulations {#simulations}

Having discussed the various methods to handle left-censored data in the previous chapter, we now turn to a simulation study in order to evaluate the strengths and weaknesses of each method in cases of differing censoring rate, sample size, and distribution. We will also discuss the implementation of the methods, data generating mechanisms, and specific evaluation metrics to assess the performance of each method.

## Aims {#aims}

The question of which method is the best to use is frequently discussed topic within the field. Many studies have been conducted over the years to evaluate the performance of these methods to handle left-censored data, with the results being widely varied and largely inconclusive. First and foremost, a large issue comes in that every conducted study widely differs in the methods being investigated and the scope of the study. As an example of the broad differences between studies which can make comparisons difficult, [@Antweiler2015] evaluates the effectiveness of 11 different methods with several censoring rates and distributional assumptions, using the median absolute deviation (MAD) as their performance metric of choice. Meanwhile, [@Hall2020] focuses instead on the applications of such methods from a water-quality focused context and investigates the performance of the four methods used in this thesis -- except with water stream concentration data, and no focus on distributional assumptions nor censoring rates. As each of these studies are concerned with their own goals -- the reasoning and conclusion that they reach will inevitably be different. Studies that are more focused on a general, broad audience, with no assumptions as to what sort of data the individual is working with -- may find more use with the conclusion and results that investigators like Antweiler come up with. There may also be individuals who are more focused on the performance of such methods in a specific context, as in the study conducted by Hall. There is no common ground between statisticians on the optimality of methods, prompting our own foray into this topic. I wish to incorporate the detailed specifications of a simulation study, while keeping it applicable towards the coal contamination water quality data that I am hoping to apply the methods to.

Through our simulation study, we wish to identify settings where a method can be effective but also those in which the methods may not be able to perform quite as well. Several investigators in this field have found issues with certain methods underperforming under certain conditions, and brings up the possibility of particular methods being more equipped than others to deal with different rates of censoring. Specifically, a study on methodologies to handling left-censored microbial risk assessment conducted by [@Canales2018] found that the substitution method seemed to work much better than expected while other methods, such as the MLE method, seemed to have trouble when applied to highly skewed data. Results from other works [@Antweiler2015] suggest that regardless of the method being utilized, obtaining reliable estimates from datasets where censoring was greater than 40% was unfeasible. This particular study also suggests that the size of the data had no influence on the result of the estimate in question. 

Claims regarding the effectiveness of methods with regards to censoring rates, distribution of data, and sample size are all highly contentious. In order to get a better idea of sense of how these claims hold up, our goal is to evaluate the validity of those claims by conducting a simulation study of our own which will put those claims into practice. We must note that we don't expect one method to be significantly above the others in terms of performance for all settings, but we do hope to see which method might be best for certain settings, in terms of distributional assumptions, censoring rates, and sample sizes. This approach and aim of our simulation study will then largely depend on how we will generate the data to be used in order to achieve our goal.

## Data-Generating Mechanisms {#data_generating_mechanisms}

The data generated for use in our study will be obtained by using parametric draws from user-specified distributions (log-normal, exponential, and Weibull), as the methods utilized can only be used with non-negative distributions [@Yavuz2017]. Our data-generating mechanism also alters criterion such as the sample size, $n_{obs} = \{10, 100, 1000\}$ and censoring rate $R = \{0.10, 0.20, 0.30, 0.40, 0.50\}$.

Censored values are generated and determined by first arranging the uncensored observations in ascending order, and then from the censoring rate. For instance, if our censoring rate were $R = 0.15$, the lowest 0.15 of the observations will be marked as censored while the rest remain uncensored.

## Estimands {#estimands}

Each of the four methods discussed in the previous chapter are designed for usage in obtaining summary statistics for left censored data [@Shoari2018]. in our simulation study, we want to evaluate just how well these methods are able to estimate a population quantity. We will be using the sample mean as an estimator for our estimand, the population mean, $\mu$.

## Performance Measures {#performance_measures}

Morris et al. (2019) define performance measures as numeric metrics used to assess the performance of the method in question. The criterions we will use to assess the performance of each of our four methods will consist of: bias, variance, and mean squared error (MSE).

### Variance

Before defining variance, it is important to have a good grasp of the concept of precision. Precision simply refers to how far away estimates from different samples are from one another. Low precision indicates that the estimates from each sample are close to one another in value and vice versa. 

Knowing this, variance is a metric which informs us on the precision of an estimator. It is defined as simply the average squared deviation of the estimator from its average, which in our case is defined as:

$Variance = E[(\hat{\mu}-E(\hat{\mu}))^2]$

Estimators with low variances generally remain close in value throughout all samples, while those with high variance may wildly differ between samples. As such, it is generally preferable to have an estimator with low variance. However, it is important to note that precision measurements, such as the variance, are not a sole indicator of an estimator's performance [@Walther2005]. While precise estimators are ideal, it is also important to assess the estimator's bias, how close it is to the true value.

### Bias

Bias is defined as the difference between an estimator's expected value and the true value of the parameter. In our case, we are using the estimator $\hat{\mu}$ to "estimate" the true population mean, $\mu$, in each of our samples. As such, bias can be defined in our case as:

$Bias = E(\hat{\mu}) - \mu$

It is important to note that bias is a metric which only informs us on the difference of the estimator from the true parameter, and tells us nothing regarding accuracy nor precision.

If the bias of an estimator were to be equal to zero, we would define the estimator to be _unbiased_, meaning that the estimator produces parameter estimates which are on average, equal to the true value.

However, it is important to note that just because an estimator is unbiased, does not necessarily tell us anything about the quality of our estimator (of being good or bad). An unbiased estimator could have high variance, which would mean that the estimator in each sample would be significantly different from one another, but on average -- they equal the true population estimand. 

On that same note, it would not be very useful if an estimator had low variance but high bias, either -- as this would mean that each sample would consistently produce similar estimates which are very far away from the true population estimand in question. 

### Mean Squared Error (MSE)

We generally would like estimators which have low bias and low variance, but it can be difficult to achieve both at once. As such, it is common to instead turn to a quantity known as the mean squared error (MSE), which is a quantitative measurement used to assess the accuracy of an estimator. The MSE measures how far away, on average, an estimator is from its true value.

(NOTE: section below is VERY messy, need to clean up, play around with math mode, blah)

$MSE = E[(\hat{\mu} -\mu)^2] = Var(\hat{\mu})+[Bias(\hat{\mu})]^2$

We can show that the MSE of estimator can be rewritten in terms of its variance and bias:

$$E[(\hat{\mu} -\mu)^2] = E(\hat{\mu}^2) + \mu^2 - 2E(\hat{\mu})\mu$$
Since we know bias to be $Bias = E(\hat{\mu}) - \mu$, it follows that $Bias^2 = E^2(\hat{\mu}) +\mu^2 -2E(\hat{\mu})\theta$. We already know variance to be $Variance = E[(\hat{\mu}-E(\hat{\mu}))^2] = E(\hat{\mu}^2) - E^2(\hat{\mu})$. Thus, combining the square of the bias with variance yields:

$Bias^2 + Var = [E^2(\hat{\mu}) +\mu^2 -2E(\hat{\mu})\theta] + [E(\hat{\mu}^2) - E^2(\hat{\mu})]$ the $E^2(\hat{\mu})$ terms cancel out, and we are left with: $E^2(\hat{\mu}) +\mu^2 -2E(\hat{\mu})\theta = E[(\hat{\mu} -\mu)^2] = Bias$.

As the MSE is always positive, MSE values closer to zero are more desirable -- as it is an indicator that the estimator is accurate.

## Results {#results}

[place figures/tables from results of simulation study here, along with explanation]

```{r libraries, include = FALSE}
library(tidyverse)
library(Metrics) #package to help calculate mse
library(NADA) #package with implementation of many methods
library(survival)
```

```{r generateLN, include = FALSE}
#function will generate a vector of numbers from the lognormal 
#distribution and censor them at the given rate
#function will take in arguments for 1) samplesize, 2) logmean, 3)logsd
#4) censoring rate

generateLN <- function(sampsize, m, s, censrate){
  true.value <- rlnorm(sampsize, 
               meanlog=log(m^2 / sqrt(s^2 + m^2)),
               sdlog=sqrt(log(1 + (s^2 / m^2))))
  
  uncensored_df <- as.data.frame(true.value) %>%
    arrange(true.value)
  
  censored_df <- uncensored_df %>% #take the head(%) of data to be censored
    slice_head(n=nrow(uncensored_df)*censrate) %>%
    mutate(censored = TRUE)
  
  #full join original df and sliced df
  return_df <- full_join(uncensored_df, censored_df, by = "true.value")

  #replace NAs with FALSE
  return_df$censored <- replace_na(return_df$censored, replace = FALSE)
  
  return(return_df)
}
```

```{r generateEXP, include = FALSE}
#function will generate a vector of numbers from the exponential 
#distribution and censor them at the given rate
#function will take in arguments for 1) samplesize, 2) rate,
#3) censoring rate

generateEXP <- function(sampsize, r, censrate){
  true.value <- rexp(sampsize, rate = r)
  
  uncensored_df <- as.data.frame(true.value) %>%
    arrange(true.value)
  
  censored_df <- uncensored_df %>% #take the head(%) of data to be censored
    slice_head(n=nrow(uncensored_df)*censrate) %>%
    mutate(censored = TRUE)
  
  #full join original df and sliced df
  return_df <- full_join(uncensored_df, censored_df, by = "true.value")

  #replace NAs with FALSE
  return_df$censored <- replace_na(return_df$censored, replace = FALSE)
  
  return(return_df)
}
```

```{r generateW, include = FALSE}
#function will generate a vector of numbers from the Weibull
#distribution and censor them at the given rate
#function will take in arguments for 1) samplesize, 2) rate,
#3) censoring rate

generateW <- function(sampsize, sh, sc, censrate){
  true.value <- rweibull(sampsize, shape = sh, scale = sc)
  
  uncensored_df <- as.data.frame(true.value) %>%
    arrange(true.value)
  
  censored_df <- uncensored_df %>% #take the head(%) of data to be censored
    slice_head(n=nrow(uncensored_df)*censrate) %>%
    mutate(censored = TRUE)
  
  #full join original df and sliced df
  return_df <- full_join(uncensored_df, censored_df, by = "true.value")

  #replace NAs with FALSE
  return_df$censored <- replace_na(return_df$censored, replace = FALSE)
  
  return(return_df)
}
```

```{r setup, include = FALSE}
iterations <- 1000 #number of iterations
censvalues <- c(0.10, 0.30, 0.50)
sampsizes <- c(10, 100, 1000)

df.tall <- data.frame(prop_cens = numeric(),
                      samplesize = numeric(),
                      iteration = numeric(),
                      method = character(),
                      true_mean = numeric(),
                      mean_complete = numeric(),
                      mean_method = numeric(),
                      true_sd = numeric(),
                      SE_complete = numeric(),
                      SE_method = numeric())
```

```{r lognormal, echo = FALSE, message = FALSE, cache = TRUE}
#LOGNORMAL
options(scipen=999) #prevent scientific notation
set.seed(7271999)

for(i in censvalues){
  for(j in sampsizes){
    for(k in 1:iterations){
      m <- 1
      s <- 0.5
      df <- generateLN(sampsize = j, m = 1, s = 0.5, censrate = i)
      
      #substitution
      #define LOD to be smallest, uncensored value
      LOD <- min(df$true.value[df$censored == FALSE]) 
      df <- df %>%
        mutate(impSubValue = if_else(censored == TRUE, LOD/2, true.value))
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "substitution",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(df$impSubValue),
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(df$impSubValue)/sqrt((length(df$impSubValue))))
      
      #mle
      mle_res = cenmle(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "mle",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(mle_res)[1],
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(mle_res)[2])
      
      #km
      km_res = cenfit(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "km",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(km_res)[[1]],
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(km_res)[[2]])
      
      #ros
      ros_res = ros(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "ros",
                true_mean = m,
                mean_complete = mean(df$true.value),
                mean_method = mean(ros_res),
                true_sd = s,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(ros_res)/sqrt((length(df$true.value))))
    }
    #end of # iterations
  }
}

#aggregating performance criteria

df.ln <- df.tall %>%
  group_by(prop_cens, samplesize, method) %>%
  summarize(Avg_Mean = mean(mean_method),
            Bias = (mean(mean_method) - true_mean),
            Variance = var(mean_method),
            MSE = mse(true_mean, mean_method) 
            ) %>%
  distinct() %>%
  ungroup()
```

```{r exponential, echo = FALSE, message = FALSE, cache = TRUE}
#EXPONENTIAL
options(scipen=999) #prevent scientific notation
set.seed(7271999)

for(i in censvalues){
  for(j in sampsizes){
    for(k in 1:iterations){
      r = 1
      df <- generateEXP(sampsize = j, r = r, censrate = i)

      #substitution
      #define LOD to be smallest, uncensored value
      LOD <- min(df$true.value[df$censored == FALSE]) 
      df <- df %>%
        mutate(impSubValue = if_else(censored == TRUE, LOD/2, true.value))
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "substitution",
                true_mean = 1/r,
                mean_complete = mean(df$true.value),
                mean_method = mean(df$impSubValue),
                true_sd = 1/r,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(df$impSubValue)/sqrt((length(df$impSubValue))))
      
      #mle
      # mle_res = cenmle(df$true.value, df$censored)
      # 
      # df.tall <- df.tall %>%
      #   add_row(prop_cens = i,
      #           samplesize = j,
      #           iteration = k,
      #           method = "mle",
      #           true_mean = 1/r,
      #           mean_complete = mean(df$true.value),
      #           mean_method = mean(mle_res)[1],
      #           true_sd = 1/r,
      #           SE_complete = 
      #             sd(df$true.value)/sqrt((length(df$true.value))),
      #           SE_method = mean(mle_res)[2])
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "mle",
                true_mean = NA,
                mean_complete = NA,
                mean_method = NA,
                true_sd = NA,
                SE_complete = NA,
                SE_method = NA)
      
      #km
      km_res = cenfit(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "km",
                true_mean = 1/r,
                mean_complete = mean(df$true.value),
                mean_method = mean(km_res)[[1]],
                true_sd = 1/r,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(km_res)[[2]])
      
      #ros
      ros_res = ros(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "ros",
                true_mean = 1/r,
                mean_complete = mean(df$true.value),
                mean_method = mean(ros_res),
                true_sd = 1/r,
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(ros_res)/sqrt((length(df$true.value))))
    }
    #end of # iterations
  }
}

#aggregating performance criteria

df.exp <- df.tall %>%
  group_by(prop_cens, samplesize, method) %>%
  summarize(Avg_Mean = mean(mean_method),
            Bias = (mean(mean_method) - true_mean),
            Variance = var(mean_method),
            MSE = mse(true_mean, mean_method) 
            ) %>%
  distinct() %>%
  ungroup()
```

```{r weibull, echo = FALSE, message = FALSE, cache = TRUE}
#WEIBULL
options(scipen=999) #prevent scientific notation
set.seed(7271999)

for(i in censvalues){
  for(j in sampsizes){
    for(k in 1:iterations){
      sh = 1
      sc = 1
      df <- generateW(sampsize = j, sh = sh, sc = sc, censrate = i)

      #substitution
      #define LOD to be smallest, uncensored value
      LOD <- min(df$true.value[df$censored == FALSE]) 
      df <- df %>%
        mutate(impSubValue = if_else(censored == TRUE, LOD/2, true.value))
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "substitution",
                true_mean = sc*gamma(1+(1/sh)),
                mean_complete = mean(df$true.value),
                mean_method = mean(df$impSubValue),
                true_sd = sqrt((sc^2)*(gamma(1+(2/sh)) - 
                                      (gamma(1+(1/sh)))^2)),
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(df$impSubValue)/sqrt((length(df$impSubValue))))
      
      #mle
      # mle_res = cenmle(df$true.value, df$censored)
      # 
      # df.tall <- df.tall %>%
      #   add_row(prop_cens = i,
      #           samplesize = j,
      #           iteration = k,
      #           method = "mle",
      #           true_mean = 1/r,
      #           mean_complete = mean(df$true.value),
      #           mean_method = mean(mle_res)[1],
      #           true_sd = 1/r,
      #           SE_complete = 
      #             sd(df$true.value)/sqrt((length(df$true.value))),
      #           SE_method = mean(mle_res)[2])
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "mle",
                true_mean = NA,
                mean_complete = NA,
                mean_method = NA,
                true_sd = NA,
                SE_complete = NA,
                SE_method = NA)
      
      #km
      km_res = cenfit(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "km",
                true_mean = sc*gamma(1+(1/sh)),
                mean_complete = mean(df$true.value),
                mean_method = mean(km_res)[[1]],
                true_sd = sqrt((sc^2)*(gamma(1+(2/sh)) - 
                                      (gamma(1+(1/sh)))^2)),
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = mean(km_res)[[2]])
      
      #ros
      ros_res = ros(df$true.value, df$censored)
      
      df.tall <- df.tall %>%
        add_row(prop_cens = i,
                samplesize = j,
                iteration = k,
                method = "ros",
                true_mean = sc*gamma(1+(1/sh)),
                mean_complete = mean(df$true.value),
                mean_method = mean(ros_res),
                true_sd = sqrt((sc^2)*(gamma(1+(2/sh)) - 
                                      (gamma(1+(1/sh)))^2)),
                SE_complete = 
                  sd(df$true.value)/sqrt((length(df$true.value))),
                SE_method = 
                  sd(ros_res)/sqrt((length(df$true.value))))
    }
    #end of # iterations
  }
}

#aggregating performance criteria

df.w <- df.tall %>%
  group_by(prop_cens, samplesize, method) %>%
  summarize(Avg_Mean = mean(mean_method),
            Bias = (mean(mean_method) - true_mean),
            Variance = var(mean_method),
            MSE = mse(true_mean, mean_method) 
            ) %>%
  distinct() %>%
  ungroup()
```

The results of our simulation study are presented in the following tables below.

\newpage

```{r, echo = FALSE}
knitr::kable(df.ln, caption = "Performance metrics of our our 4 methods with data derived from the log-normal distribution with mean = 1 and SD = 0.5.", digits = 5)
```

\newpage

```{r, echo = FALSE}
knitr::kable(df.exp, caption = "Performance metrics of our our 3 methods (MLE method absent) with data derived from the exponential distribution with a shape parameter = 1.", digits = 5)
```

\newpage

```{r, echo = FALSE}
knitr::kable(df.w, caption = "Performance metrics of our our 3 methods (MLE method absent) with data derived from the Weibull distribution with a shape parameter = 1 and scale parameter = 1.", digits = 5)
```

From the results of our simulation study, we can see that with the data generated from the log-normal distribution, in the case of low censoring (0.10), when dealing with sample sizes of 10 and 100, the methods are largely comparable to one another. Substitution and KM do not perform quite as well as ROS and MLE, both displaying an increase in absolute bias and MSE when compared to the latter two. Substitution performs significant worse than KM. MLE and ROS perform rather equally well in all sample sizes for low-censoring. 

When considering medium censoring (0.30), much of the same observations still hold true. Substitution performs the worse, followed by KM. MLE and ROS both perform well. However, ROS has a slight edge over MLE, especially as sample sizes increase, attaining lower MSE values than the latter.

All four methods begin to perform worse when the censoring rate is increased to 0.5, which is to be expected. As more and more missingness is introduced within the dataset, it becomes more difficult to obtain accurate estimates for all methods. Once again, substitution and KM attain high absolute bias and MSE values. However, it is now KM which performs worse than substitution in the setting of high censoring. Similarly to before, albeit being more noticeable now, ROS performs better than MLE with all sample sizes.

We can see with the exponential and Weibull cases, all three methods perform equally well in the case of low (0.10) censoring with all sample sizes, obtaining similar bias and MSE values across all sample sizes for both the exponential and Weibull datasets. KM consistently performs the worst with with medium (0.30) and high (0.50) censoring when compared to the other methods across all sample sizes. In these censoring settings, it is also the case that ROS performs the best with substitution not far behind.

In summary, regardless of distributional assumptions all of the methods perform well when censoring is low, with very minute differences in performance metrics. KM does not perform well in the lognormal distribution with high censoring rates and struggles in the exponential and Weibull cases with medium and high censoring rates. While MLE was only used in the case of the lognormal data, it also performs quite well, although not quite as well as ROS. ROS performed the best in the case of medium and high censoring across all 3 distributions.

## Discussion {#discussion}

[discuss findings from the simulation study. are the results expected from knowledge gained from literature search? are they different? compare/contrast with literature. what things seem to be shared between our findings and that in literature?

### Limitations {#limitations}

[discuss some limitations of the simulation study -- ideas include things such as how simulated data =/= real life data, discuss some limitations, future plans?]

Shortcomings in the results presented in this study very well may come from the fact that we generated data with known distributional parameters. It could be the case that the effectiveness of our methods were only due to having such artificial data. Alterations in our study to instead generate data from methods such as randomized pulls from an a real-world dataset of interest via. methods such as bootstrapping could provide different insights.

## Study on Real Data {#real_data}

[connect back to chapter 1]

[can write this chapter after some preliminary exploration with coal groundwater data]
