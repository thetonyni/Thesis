---
output:
  pdf_document: default
  html_document: default
---

```{r load_packages2, include = FALSE}
library(mosaic)
library(knitr)
```

# Introduction {#intro}

The concept of missing data is ubiquitous across academic disciplines and often complicates real-world studies. Most studies utilize data collected through surveys, questionnaires, and/or field research which is why missing data is often unavoidable. Missing data can hinder one's ability to work with and analyze the phenomena at hand, giving rise to inaccurate or even misleading analyses.

@Barnard1999 outline several significant issues when conducting analysis on missing data. Firstly, missing data can introduce bias in regards to parameter estimation. It can also lead to a reduction in statistical power, which can affect the conclusions one makes during studies involving hypothesis testing. Finally, missing data can introduce complications with statistical software and lead to functions not working as intended, if they have not accounted for the possibility of the data containing missingness.

This thesis will go into a more specific instance of missing data known as censoring, which is _the condition when one has only partial information regarding the values of a measurement within a dataset_. In this chapter, we will introduce and define the three types of censored data, discuss the challenges with the reporting of censored data, and explore common statistical approaches to handling censored data.

## Censored Data {#censored_data}

As discussed previously, censored data is a specific type of missingness where one has only partial information regarding the values of a measurement in a dataset. There are three types of censoring which can occur: right censoring, interval censoring, and left censoring.

### Three Types of Censoring {#right}

Right censoring is a specific instance of censoring in which we only know that the true value of a data point lies _above_ a certain threshold, but it is unknown by how much. This is the most common type of censoring and can often be found in clinical trial studies, mortality studies, and other forms of survival analyses.

In contrast with right censoring, in the case of left censoring, we only know that the true value of a data point falls _below_ a certain threshold which we call the _limit of detection_ (LOD), and similarly to right censoring, it is unknown by how much. Left censoring is commonly found in environmental, water quality, and chemical-related research where the focus is on the concentration of an analyte.

Right and left-censoring are both special cases of interval censoring. Interval censoring is when the random variable of interest is known to be between an interval of two values. Considering a random variable $T$, which denotes the survival time of interest. If interval censoring present, we can denote the interval containing $T$ to be $I = [t_1, t_2]$, with $t_1$ being the beginning of the interval and $t_2$ being the end of the interval. In the case of left censoring, $t_1 = 0$; and conversely in the case of right censoring, $t_2 = \infty$.

<!--Suppose a study on income and mortality is conducted with the variable of interest, $T$, being the time measured from the start of the study to the death of the participant. The study has a duration of 5 years, in which participants are expected to submit a form regarding their annual income. The value for the participant would be considered to be right-censored if at any point during the study, they failed to follow-up, or if the participant was still alive at the conclusion of the 5 year study. In this design study, several possibilities can occur, illustrated in Figure \@ref(fig:rightcensoringexample).

```{r, echo = FALSE, rightcensoringexample, fig.cap="Right Censoring Example", out.width = '100%'}
knitr::include_graphics(path = "figures/right_censoring_example.png")
```

As illustrated by individual A in Figure \@ref(fig:rightcensoringexample), this individual lives on until the termination of the study. We don't know at what point they passed away exactly, since they didn't pass away during the time constraints of the study. As such, the only information we have is $T > 5$.

If an individual does pass away at some point, $t_i$, _during_ the study, then $T = t_i$. This can be illustrated within Figure \@ref(fig:rightcensoringexample) by individuals B, C, and D for which $T = 3$, $T = 4$, and $T = 1$.

There is a final possibility for individuals who choose to censor themselves. Illustrated in Figure \@ref(fig:rightcensoringexample) by individuals E and F, we can see that they are marked as censored at $T = 3$ and $T = 1$, respectively. These individuals may have chosen to stop submitting information to the study or drop out of the study entirely without warning. As we have no information about whether or not if they died or simply did not submit their form, all we know is that the individual died/will die at some point after the point at which they were censored.

Right censoring is the most common type of censoring and can often be found in clinical trial studies, mortality studies, and other forms of surival analyses.
-->

### Left Censoring {#left}

```{r, leftcensoringexample, echo = FALSE, fig.cap="Left Censoring Example", out.width = '100%'}
knitr::include_graphics(path = "figures/left_censoring_example_fix.png")
```

To clarify the concept of left-censoring, consider the following example illustrated with Figure \ref{fig:leftcensoringexample}. Imagine a scenario in which you are attempting to estimate the time at which the sun rises each morning. You plan to wake up every morning far before the sun rises, but on the first day of the study, you oversleep and wake up at 7:00 A.M. with the sun already out. We now have an instance of left-censored data. We want to know the time at which the sun rose, but all we have is an upper limit (7:00 A.M.). As the focus of my thesis, left-censoring poses many challenges in regards to how individuals should report and work with left-censored data.

<!--

### Interval Censoring {#interval}

```{r, echo = FALSE, intervalcensoringexample, fig.cap="Interval Censoring Example", out.width = '100%'}
knitr::include_graphics(path = "figures/interval_censoring_example_fix.png")
```

Interval censoring is another form of censoring in which the random variable of interest is known to be between an interval of two values. Considering a random variable $T$, which denotes the survival time of interest, if interval censoring is at hand, we can denote the interval containing $T$ to be $I = [t_1, t_2]$, with $t_1$ being the beginning of the interval and $t_2$ being the end of the interval. Left and right censoring are special cases of interval censoring. In the case of left censoring, $t_1 = 0$; and conversely in the case of right censoring, $t_2 = \infty$.

To conceptualize interval censoring, we can consider a example study on virus testing in which participants get their blood drawn in order to detect whether or not they test positive for a virus or not. The random variable in question is $T$, which represents the exact timepoint at which the subject contracted the virus. If an individual was first tested at time $t_1$ and tested negative, but was tested again at a later time $t_2$ and tested positive, the specific time $t$ at which the subject contracted the virus is unknown. All we know is that it lies somewhere between the interval, $I = [t_1, t_2]$, but not the exact time at which they contracted it.

-->

## Challenges of Reporting Censored Data {#challenges}

The most pressing issue in regards to the reporting practices for left-censored data is the lack of a universal standard as to how to report values below the LOD, which can lead to confusion amongst researchers. This lack of standardization makes it difficult to distinguish values below the LOD and uncensored values, which can lead to values below the LOD unintentionally being overlooked, causing faulty analysis or conclusions which are heavily flawed.

In a study involving the precision of lead measurements near concentrations of the limit of detection, @Berthouex1993 discusses the disparity among chemists regarding practices involving the recording values below the LOD. He enumerates the following list.

\begin{enumerate}
  \item Reporting the letters ND, ``not detected''
  \item Reporting the numeric value of the LOD
  \item Reporting ``< LOD'', where LOD is the numeric value of the LOD 
  \item Reporting some value between 0 and the LOD, such as one-half the LOD
  \item Reporting the actual measured concentration, even if it falls below the LOD
  \item Reporting the actual measured concentration, followed by ``(LOD)''
  \item Reporting the actual measured concentration with a precision ($\pm$) statement
\end{enumerate}

According to @Gilbert1987, the latter three methods are the best procedures to follow, especially from a practical and statistical point of view. He argues that assuming the small concentration values are not from some sort of measurement error during data collection, then the measured concentration holds value. As such, recording a measurement as "below LOD" without any sort of accompanying value would be discarding useful information which could have been used in practice and analysis.

@Berthouex1993 discusses the prevalence in regards to the practice of censoring data by reporting only values which are above the detection limit and discarding those which fail to yield quantifiable results. In the study he conducted, five laboratories were assigned tasks to measure samples of a certain solution. The laboratories were not given information regarding the intent of the study, but a general statement that the concentrations being measured were of "low" concentrations. All but one laboratory recorded the actual measured concentrations even though they fell below the LOD. Fortunately, the original measurements for the laboratory that did not report values for all samples were maintained and able to be recovered. @Berthouex1993 stresses the importance of standardization in reporting practices for laboratories and suggested reporting all measurements accompanied with some precision statement, so that data is not lost.

Further supporting the stance of keeping all concentration measurements rather than only those above the detection limit, Monte-Carlo experiments were conducted by @Gillom1984 to investigate trend-detection for water-quality data. Trend detection is the practice of determining whether the values of a random variable generally increase or decrease over a period of time. They found a general relationship of decreasing trend detection percentages with increased censoring levels, attributing this to the limited availability of information in censored data. 

<!-- In an article published by the American Bar Association, a company seeking to purchase land for residential use, hires several laboratories to conduct analyses of the chemical concentrations at that site. However, miscommunication occurs between the two parties, in which the laboratory mistakenly is given information that the site is used for industrial purposes. As it turns out, detection limits for residential areas differs from industrial ones, and as a result, the analytic results turned out useless for the company. This situation is a clear example of how [@Elias1999]. -->


## Parameter Estimation for Left-Censored Data {#Approaches}

It is important to note that the values below the LOD still contain information, specifically that the values is between the lower bound value (if it exists) and the LOD [@Chen2011]. As such, there are a variety of statistical treatments to handle censored data which have been popularized in the statistical literature.

Before we discuss the techniques which have been popularized through literature and studies regarding parameter estimation with left-censored data, it important to discuss *omission*, the deletion of data points which are deemed to be invalid as a result of left-censoring or any other deficiencies in the data. As a result of being simple to comprehend and implement, omission is a common technique used in lieu of specialized techniques designed to handle missing data. 

One type of omission is known as *available-case analysis*, in which statistical analysis is conducted while only considering the observations which have no missing data on the variables of interest, and excluding the observations with missing values [@May2012]. May argues against this approach and claims that the loss of information from discarding data and the inflation of standard errors of estimates (when discussing missingness in a regression context) will invariably be inflated as a result of the decreased sample size.

Over the past century, a myriad of methods to deal with censoring have been developed to counter this issue of discarding data with omission-based techniques -- some more statistically sound than others. We will review some of the more common methods to estimate descriptive statistics involving censored data, which include: substitution, maximum likelihood estimation, Kaplan-Meier, and regression on order statistics.

These methods can be classified as either imputation-based techniques, in the case of substitution and ROS, or parameter-estimation techniques, in the case of MLE and KM.

### Substitution {#Substitution} 

The first technique we will discuss is the substitution method, which involves imputing in a replacement value in lieu of the censored data point. As a method commonly condemned in papers as a statistically unsound method to handle censored data, substitution methods are ubiquitous in the chemical and environmental sciences [@Canales2018].

Substitution techniques are easy to understand and to implement, akin to the omission techniques we discussed previously. Observations for which measurements fall below the LOD are replaced with a replacement value, which is non-specific and can vary between studies. A non-exhaustive list of common replacement values include: $\frac{LOD}{2}$, $\frac{LOD}{\sqrt2}$, and the $LOD$ itself [@Lee2005]. 

Proponents of the substitution method claim that the replacement value $\frac{LOD}{2}$ is useful for data sets in which the majority of the data are below the LOD or when the distribution of the data is highly skewed; the definition of "highly skewed" being any distribution with a geometric standard deviation (a measure of spread commonly used in tandem with log-normal distributions) of 3 or more [@Hornung1989]. They also suggest using $\frac{LOD}{\sqrt2}$ when there are only a few data points below the LOD or when the data is not highly skewed.

Regardless of which replacement value is used, once a dataset is generated containing these replacement values, analysis continues as is, utilizing this new dataset. 

As mentioned previously, contention between whether this method is statistically sound or not remains to the present day. The lack of a global, standardized replacement value to substitute is one of the most pronounced downsides of this method.  Different disciplines have their own suggested "best" replacement value to use, an example being $\frac{3}{4}$ times the LOD being a common replacement value in geochemistry [@Crovelli1993]. Due to the lack of standardization, many regard substitution techniques as a non-rigorous, statistically unsound method for handling left-censored data [@Chen2011].

@Lee2005 also provides a critique on substitution methods and claim that they can often introduce a "signal" which was not originally present within the data, or even obstruct an actual signal which was originally present in the data -- leading to misleading and/or inaccurate results. Supporting @Lee2005's claim, Glass and Gray (2001) found the substitution method to introduce large errors and biases when calculating descriptive statistics of interest with left-censored data. Thompson and Nelson (2001) conducted a study in which they found similar results, in that it often led to biased parameter estimates and "artificially small standard error estimates." Hewett and Ganser (2007) also found in their simulation study that the substitution method yielded the lowest average bias and root mean squared error values (comparison metrics to measure accuracy) in their estimation of the mean. Overall, the overall consensus seems to advise against the practice of these substitution techniques.

### Maximum Likelihood Estimation {#MLE}

Maximum likelihood (ML) estimation is a parametric technique which allows us to estimate the parameters of a model when the data are from a known distribution.

Let the random variables $X_1, \cdots, X_n$ be independent and identically distributed with probability function $f(x_i|\theta)$, where $\theta$ is the parameter we are interested in estimating. For every observed random sample $x_1,\cdots,x_n$, the joint density function is:

$$f(x_1,...,x_n|\theta) = f(x_1|\theta)...f(x_n|\theta) = \prod_{i=1}^{n}f(x_i|\theta)$$

Our goal is to find the value of $\theta$ which is most likely to generate our observed data. In order to solve this inverse problem, we introduce the likelihood function, which is defined as a function of the parameter given the observed data:

$$L(\theta) = L(\theta|x_i, \cdots, x_n) = \prod_{i=1}^{n}f(x_i|\theta)$$
Our *maximum likelihood estimate* of our parameter $\theta$, then, is the value $\hat{\theta}$ that maximizes the likelihood function, $L(\theta)$.

When left censoring is present, the likelihood function changes in order to account for both the censored observations and the uncensored observations. We define $F(x_i|\theta)$ to the cumulative distribution function for our RVs conditioned on $\theta$. Our new likelihood function when left censoring is present is:

$$L(\theta) = \prod_{i=1}^n f(x_i|\theta)^{\delta_{i}} \ \times F(x_i|\theta)^{1-{\delta_{i}}}$$

\noindent where $\delta_{i}$ indicates whether or not the $i$th observation is censored:

$$\delta_i =
\begin{cases}
  0 & \text{if censored} \\
  1 & \text{if uncensored}
\end{cases}$$

It is then possible to follow typical procedures to find the maximum likelihood estimates of the parameteres of interest (mean, variance, etc.) from our censored data.

@Yavuz2017 discusses the usage of ML estimation when missing data is present, and notes it is only appropriate for non-negative probability distributions such as the exponential, log-normal, and Weibull models. This is one limitation of ML estimation, it cannot be applied for data which do not fit a specified model -- and is very limited in scope.

ML estimation is one of the most well-known parametric approaches to handling left-censored data. Many studies use ML estimation as a baseline method of handling censored values, to which they compare their new techniques [@Ganser2010]. Despite its prevalence, ML estimation has its weaknesses. @Canales2018 found that the ML estimation seems to underperform when the data in question was highly skewed, producing overinflated mean squared errors. Additionally, because ML estimation is so heavily dependent upon distributional assumptions, an incorrect specification of the distribution of the censored data will inevitably lead to misleading results [@Bolks2014]. Regardless of these limitations, it is a definite staple in the field of parameter estimation with regards to censored data.

### Kaplan-Meier {#rkm}

As a phenomenon, censoring is most often discussed in survival analysis, which concerns itself with techniques to analyze a _time to an event_ variable. As its name suggests, these variables measure the time which passes until some event of interest occurs. This can be as innocuous as the time until a device breaks, time until birds migrate away from their homes, time until a person passes away, etc. In all cases, there is a possibility of the data being censored.

The Kaplan-Meier (KM) method is a common nonparametric technique used to deal with censored data. Nonparametric methods do not make assumptions about the underlying distribution of the data. The KM method was originally developed to handle right-censored survival analysis data [@Hall2020]. The advantages of the KM method lies in its robustness as a nonparametric method -- it performs well without having to depend upon distributional assumptions. Many recommend its usage in cases of severe censoring, instances where more than 90% of the data is censored [@Canales2018].

<!--

* [picture of survival cureve? tentative...]

-->

The KM-estimator is a statistic used to estimate the survival curve from the data while accounting for censoring. It does this by assuming that censoring is independent from the event of interest and that survival probabilities remain the same in observations found early in the study and those recruited later in the study [@Gillespie2010].

The KM-estimator of the survival curve at time $t$ is:

$$\hat{S}(t) = \prod_{\ t_i \le \ t }\left(1-\frac{d_i}{n_i}\right)$$

where $t_i$ is the distinct event time, $d_i$ is the number of event occurrences at time $t_i$, and $n_i$ is the number of followup times ($t_i$) that are greater than or equal to $t_i$ (how many observations in sample survived until at least time $t_i$) [@Klein2003].

Typically, the KM-estimator is used to estimate the distribution function of right-censored data. Helsel (2005, as cited in Yavuz et al., 2017) provided a simple modification of the KM-estimator to allow for the estimation of the survival curve with left-censored values. In his implementation, he ran the left-censored data through a transformation algorithm before using the KM method to change them into right-censored data. 

The transformation algorithm works as follows: First, all the left-censored values in the dataset are arranged in descending order of magnitude. Then these left-censored values are subtracted from $M$, to get $M-x_i$, the newly transformed, right-censored value ($M$ is a constant bigger than the maximum value in the dataset). Finally, the non-censored values and the newly transformed values are then arranged in ascending order to be used to estimate the survival function through the Kaplan-Meier estimator.

The KM-method is not an imputation procedure, but instead an estimation technique that allows for the calculation of descriptive statistics for left-censored datasets. @She1997 gives the expressions to calculate the estimated mean, median, and variance:

\begin{table}[h]
\begin{center}
\begin{tabular}{ll}
\textbf{Descriptive Statistic} & \textbf{Expression}                                                                                              \\
Mean ($\hat{\mu}$)             & $\hat{\mu} = \displaystyle \int_{0}^{\infty} \hat{S}(t) \ dt$                                                    \\
Median ($\hat{M}$)             & $\hat{M} = \displaystyle \hat{S}^{-1} \left (\frac{1}{2} \right)$                                                              \\
Variance ($Var(\hat{\mu}$)     & $Var(\hat{\mu}) = \displaystyle \sum_{i=1}^{r} \left( \int_{t_i}^{\infty}\hat{S}(t) \ dt \right)^2 \frac{d_i}{n_i(n_i - d_i)}$
\end{tabular}
\caption{\label{tab:KMDesc}Three descriptive statistics for the KM-method.}
\end{center}
\end{table}


### Regression on Order Statistics {#ROS}

Lastly, regression on order statistics (ROS) combines both the parametric nature of the MLE approach and nonparametric nature of the KM method. ROS is a semi-parametric method which assumes an underlying distribution (usually lognormal) for the censored measurements but makes no assumption towards the distribution of uncensored measurements.

<!--
@EPA2009 provides a a more detailed explanation to the methodology of ROS, but the basic procedures will be outlined in this thesis. 

A brief explanation as to how ROS works is as follows. ROS begins with the estimation of the cumulative probability associated with each distinct LOD within a variable. This cumulative probability is distributed equally between the censored values with a common LOD. Once the censored values are ranked, a linear regression model is fit between the uncensored values and the distributional z-scores from the censored probability plot. The parameters of the regression model (slope and intercept of the regression line) are then used to estimate the mean and standard deviation of the distributional model which are then used to generate imputed values for the censored observations.
-->

ROS begins with estimation of the cumulative probability of each uncensored value. Then, these cumulative probabilities are used to calculate z-scores from the normal distribution. We then plot the values against these z-scores, an example visualization can be seen in Figure \ref{fig:rosexample}. The uncensored observations are regressed on their corresponding z-scores. If needed, the uncensored values are transformed to better satisfy linearity of the relationship. Finally using the regression line, we use the z-scores for the censored observations to predict an estimated value.

```{r, rosexample, echo = FALSE, fig.height = 3, , fig.cap="An example visualization of the censored probability plot, from ROS.", out.width = '100%'} 
values <- c(16, 18, 19, 23, 24, 25, 25, 28, 29, 29, 30, 30, 32, 33, 35)
censored <-c("Censored", "Censored", "Censored", "Uncensored",
             "Uncensored","Uncensored","Uncensored","Uncensored",
             "Uncensored","Uncensored","Uncensored","Uncensored",
             "Uncensored","Uncensored","Uncensored")
zscores <- c(-1, -0.8, -0.5, -0.46, -0.44, -0.32, -0.05, 
             0.09, 0.2, 0.27, 0.34, 0.5, 0.7, 1.0, 1.5)

df <- cbind.data.frame(values, zscores, censored)

ggplot(df, aes(x = zscores, y = values, color = censored)) + 
  geom_point(size = 2) +
  labs(x = "Z-Score", y = "Value", color = "Censoring Status") +
  ggtitle("Example Censored Probability Plot") +
  theme_bw()
```

Delving into the mathematics behind this procedure, we need to first define several variables.

Let $LOD_k$ be the $k^{th}$ distinct LOD value for a variable with $K$ distinct limits of detection. $A_k$ as the number of uncensored values between the $k$th and $(k+1)$th LOD for $k=1$ to $K-1$, and $A_k$ as the number of uncensored values above the highest LOD, $A_0$ as the number of uncensored values below the lowest LOD.

We also define $B_k$ as the total number of observations (both uncensored and censored) with values below the $k$th LOD and $B_0 = 0$.

We can then write the number of censored values below the $k$th LOD ($C_k$) as:

$$\textrm{Eq. 1.1:} \ \  C_k = B_k - B_{k-1}-A_{k-1}$$ 

Using these values we can calculate what is known as the _exceedance probability_ ($p^e_{k}$) for each of our $k$ LOD values, defined recursively as:

$$\textrm{Eq. 1.2:} \ \  p^e_k = p^e_{k+1} + \frac{A_k}{A_k+B_k}(1 - p^e_{k+1})$$
where $p^e_0 = 1$ and $p^e_{k+1} = 0$.

The cumulative probability associated with the $i^{th}$ uncensored value between $LOD_k$ and $LOD_{k+1}$ is then:

$$\textrm{Eq. 1.3:} \ \  p^u_{kj} = (1-p^e_k)+ \left(\frac{j}{A_k+1}\right)*(p^e_k-p^e_{k+1})$$

for $j = 1$ to $A_i$ and for $k = 0$ to $k$.

Similarly, the cumulative probability associated with the $j^{th}$ censored value, censored at $LOD_k$ is:

$$\textrm{Eq. 1.4:} \ \  p^c_{kj} = \left(\frac{j}{C_k+1}\right)(1-p^e_k)$$

for $j = 1$ to $C_k$ and for $k = 1$ to $K$.

With these prerequisite variables and expression defined, we can begin outlining the general procedure for obtaining parameter estimates using ROS.

1. Compute normal quantiles (z-scores) by computing $z_{kj}^{u} = \Phi^{-1}(p^u_{kj})$, where $\Phi^{-1}$ is the inverse standard normal CDF.

2. Construct censored probability plots using the z-scores from step 1.

3. If a linear trend is not visible, we may want to plot $z_{kj}^{u}$ against a transformation, $f(\cdot)$, (ex: log, square root, inverse, etc.) of the uncensored values $x_{kj}^{u}$.

4. Compute correlation coefficient between the $z_{kj}^{u}$ and $f(x_{kj}^{u})$ pairs. The transformation which yields the highest correlation coefficient is the one which most optimizes the left-censored values.

5. Compute a linear regression of the transformed values, $f(x_{kj}^{u})$ against the z-scores, $z_{kj}^{u}$.

6. Use Eq. 1.4 to compute $p^c_{kj}$ and repeat steps 1 through 3, to obtain z-scores for the censored values.

7. Obtain the transformed imputed values using the slope and intercept from the regression model in step 10 and the censored z-scores from step 11 using the following expression:

$$f(\hat{x}_{kj}^c) = {Intercept} \ + Slope \ * z_{kj}^{c}$$

8. Combine the transformed imputed values for the censored values with the transformed uncensored values to obtain estimates for the population mean and standard deviation as normal.

As we have now finished discussing how the ROS method works, we must turn to the limitations of the ROS method. In order for ROS to be utilized, there needs to be at least 5 known values and more than half the values within the censored variables must be known. As regression is utilized in this method, the response variable must also be a linear function of the explanatory variable (quantiles). Additionally, the errors should have constant variance [@Lee2005].

## Aims {#aims}

Several studies have been conducted over the years to evaluate and compare the performance of different methods for handling left-censored data, with largely differing results and thoughts on the performance of these methods.

@Hall2020 applied each of the four methods to a water quality dataset and found that each method obtained similar estimates of the mean except for MLE which produced different (usually higher) estimates than the other methods.

@Antweiler2015 compared estimates of the median absolute deviation across these four methods (and several more) with varying censoring rates and distributional assumptions. Their study also suggested that the substitution method actually led to estimates that were not far off from the other methods, especially in the case of low censoring (<25%). They also claim that regardless of the method or sample size, obtaining reliable estimates from datasets where censoring was greater than 40% was unfeasible. 

Finally, @Canales2018 performed a study using simulated water quality data and found MLE to be ineffective, especially with highly skewed data. Similarly to @Antweiler2015, they found that the substitution method actually outperformed both MLE and KM.

As we can see, there is no common ground between statisticians on the optimality of methods, prompting our own foray into this topic. Claims regarding the effectiveness of methods with regards to censoring rates, distribution of data, and sample size are all highly contentious. In order to get a better idea of sense of how these claims hold up, our goal is to evaluate the validity of those claims by conducting a simulation study of our own to put these methods into practice, comparing and contrasting the effectiveness of our four techniques with regards to censoring rate and sample size. I wish to incorporate the detailed specifications of a simulation study, taking into consideration distributional parameters of the data-generating mechanism, censoring rates, and sample sizes for each run, while also keeping it applicable to our case-study specific data.

<!--

Regarding the simulation study that we will conduct in the following chapter, we wish to identify settings where a method can be effective but also those in which the methods may not be able to perform quite as well. Several investigators in this field have found issues with certain methods underperforming under certain conditions, and brings up the possibility of particular methods being more equipped than others to deal with different rates of censoring.

-->

