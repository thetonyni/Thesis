---
title: 'My amazing  title'
author: 'Tony Ni'
date: 'April DD, 20YY'
institution: 'Amherst College'
advisor: 'Brittney Bailey'
# Change department to'Special Programs' if interdisciplinary thesis
department: 'Mathematics and Statistics'
degree: 'Bachelor of Arts'
knit: "bookdown::render_book"
output: 
  acthesis::thesis_pdf: default
# include table of contents, list of tables, list of figures
toc: true
lot: true # set to false if no tables present
lof: true # set to false if no figures present
fig_height: 3.5
fig_width: 5.5
# Update abstract and acknowledgments filenames below as needed
abstract: |
  `r if(knitr:::is_latex_output()) 
    paste(readLines("00a-abstract.Rmd"),
    collapse = '\n  ')`
acknowledgments: |
  `r if(knitr:::is_latex_output()) 
    paste(readLines("00b-acknowledgments.Rmd"), 
    collapse = '\n  ')`
# Update the .bib filename below as needed
bibliography: bib/library.bib #change
csl: csl/apa.csl
# Uncomment the following to include any additional LaTeX packages
# header-includes:
# - \usepackage{latexPackageName}
---

<!-----------------------------------------------------------------------------
IMPORTANT REMINDERS!!

(1) Before you knit the document, make sure to update your _bookdown.yml file with:
    - your knitted pdf name (book_filename: "FirstName-LastName_StatThesis")
    - your filenames in order (rmd_files: ["thisfilename.Rmd", "firstChapter.Rmd", ..., "99-references.Rmd"])

(2) You can rename any of the chapter files, including this index file, as long as you update the filenames in _bookdown.yml

(3) Knit this file to knit the entire thesis document, or knit within a single chapter file to only knit that chapter (all other chapters will be knit with just a placeholder sentence). You will likely run into issues when knitting a single chapter if that chapter depends on code from previous chapters.  

(4) No thesis code or text should go in this index file except that used to set up the remainder of the document.
------------------------------------------------------------------------------->


<!--
The acthesis package does not need to be loaded for your thesis (but DOES need to be installed on your machine). It's loaded here as a quick way to load the packages needed to knit this example document. 

You can delete the following code chunk from your final document if you wish.
-->
```{r include_acthesis, include = FALSE}
# acthesis package loads dpylr, ggplot2, knitr, bookdown, and remotes
require(acthesis)
```


<!--
R has some built-in options that may be useful. Some recommended values are set below. Feel free to play around with the options You may edit or delete this code chunk if you wish.

- width: max width of R output (default: based on console width)
- digits: number of significant digits to print (default: 7)
-->
```{r set_options, include = FALSE}
# Default width of R ouput
options(width = 65, digits = 3)
```


<!--
This is a clunky workaround to wrap super long R output. This code chunk defines a new code chunk option called linewidth that allows you to wrap R output that falls into the margins or off the page.

The default linewidth for R output in the thesis template is 65, set with options(width = 65). If the linewidth option is still needed for a specific code chunk despite this setting, we recommend using linewidth = 65 for consistency.
-->
```{r wrap-hook, include = FALSE}
# Define a new code chunk option 'linewidth' for wrapping unruly R output
hook_output = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n, exdent = 3, indent = 3)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
# Note: set linewidth = 65 when needed.
```


<!--
The following code downloads the latest Amherst thesis cover sheet if it doesn't already exist in your folder. You may delete this code if you wish.

Amherst Thesis Guidelines are provided at https://www.amherst.edu/academiclife/registrar/for-students/thesis_guide
-->
```{r get_coversheet, include = FALSE}
# download latest thesis cover sheet
if(!file.exists("CoverSheet.pdf")){
  download.file("https://www.amherst.edu/system/files/media/Thesis%2520Copyright.pdf", "CoverSheet.pdf")
}
```



<!-- 
If knitting to HTML, the following code is required to number equations in HTML files. You may delete the following code from your final document if you wish, but it should not affect your work.
-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--chapter:end:index.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Introduction {#intro}

<!--
Each chapter file must start with a chapter header (e.g., # Chapter name). Notice we have also included a label after the chapter name ({# intro}) so we can refer back to this chapter later in the text if want to (e.g., by using \@ref(intro)). This also helps you debut potential issues as you knit.

You can knit chapters all at once using the index file, or individually. When you knit a single chapter, every other chapter will have a one-sentence placeholder to make knitting easier.

You can re-name each chapter file (e.g., change this filename from "01-chap1.Rmd" to "01-introduction.Rmd"), just make sure you also update the filename in the list of rmd files in _bookdown.yml.
-->

The introduction should provide an overview of the work you set out to do and provide structure for the remainder of the document.

## Background {#background}

* (coal ash report - car) coal one of the most dangerous combustible fossil fuels is comprised of a long list of dangerous chemicals -- including substances such as arsenic, radium, other carcinogens, metals that can impair developing children's brains, toxins dangerous to aquatic life, etc [@Kelderman2019]

* power plants produce 100mil tons of coal ash every year, which is dumped into landfills and waste ponds [@Kelderman2019]

* only recently (2015) have complaints and lawsuit arisen in which certain ecological organizations have attempted to sue the EPA to regulate disposal of coal ash [@Kelderman2019]

* this coal ash rule has forced power companies to make publicly available data regarding chemical concentrations in 265 coal plants containing ponds and landfills (about 3/4 of all coal power plants across the US) [@Kelderman2019]

* environmental agencies have concluded that the groundwater under basically all coal plants are contaminated [@Kelderman2019]

* HOWEVER this might be overstated? we wanted to investigate whether or not if this was true.

<!-- insert picture here -->

```{r, echo = FALSE, upgradientdowngradient, fig.cap="Difference Between Upgradient and Downgradient Wells", out.width = '100%'}
knitr::include_graphics(path = "figures/upgradientdowngradient.jpg")
```

* upgradient wells (background wells) measures groundwater chemical levels BEFORE passing through a coal ash dump while downgradient wells monitor the groundwater AFTER it passes through an ash dump

* we have reason to believe that many chemicals are NATURALLY OCCURING and as such, the statement made my environmental agencies regarding all groundwater being contaminated may be overstated

* typically, we would estimate the amount of chemical contamination, for this example -- arsenic -- caused by a coal ash dump with the equation: downgradient arsenic concentration minus upgradient arsenic concentration

* however, because there may be retired/unregulated upgradient wells that are occasionally contaminated already, this might be inaccurate

<!-- mayhaps remove the stat lingo BELOW -->

* END GOAL IS TO CORRECT THE CONTAMINATED VALUES

* we have already conducted a preliminary investigation using a variety of machine learning techniques to aid us in identifying potential contaminated upgradient wells

* we have also utilized bootstrapping and imputation techniques to correct for their measurements through by accounting for the innate contamination which may be caused by factors such as retired and unregulated wells

* our methodologies have yet to account for another problem however, involving limit of detection problem which arises from the measuring devices' inability to obtain chemical concentrations smaller than a certain threshold amount

## Data {#data}
<!-- 
WHERE THE DATA CAME FROM
whichever website (talk about environmental integrity project) and what they did to get this data (use xlsx for this)
this data was publcicaly available, but EIP collated it someway to make it public
the data is NOT just a single of coalset, give examples of contaminants that it’s taking measurements on
many sites within sites
-->

### Coal Ash Rule {#coalashrule}

<!-- EXPLAIN COAL ASH RULE HERE AND HOW THE EIP (Environmental Integrity Project) HAS HELPED MAKE THIS RULE -->

* A large coal ash spill at the Tennessee Valley Authority (TVA) which occured on December 22, 2008 in Kingston, TN -- prompted the Environmental Protection Agency (EPA) to propose a set of standardized regulations and procedures to address the concerns regarding coal ash plants nationwide in the US  [@Car2020]

* This was known as the Coal Ash Rule, passed on December 19, 2014 [@Car2020]

* Changes were made to the Coal Ash Rule over the years in the form of 'amendments,' one of which made required facility information and data to be made publicaly available to the public (April 15, 2015 rule change) [@Car2020]

### Source of Data

* the data used in the study are from the results published in "Annual Groundwater Monitoring and Corrective Action Reports" which were made available to the public in March 2018  [@EIP2020]

* these reports are in PDF format and are thousands of pages long, which makes it difficult for individuals to look through the data in a meaningful way [@EIP2020]

* the EIP wranged the data into a more accessible machine-readable format which contains information from over 443 annual groundwater monitoring reports posted by 265 coal ash plants [@EIP2020]

* they obtained the data from an online, publicly available database containing groundwater monitoring results from the first "Annual Groundwater Monitoring and Corrective Action Reports" in 2018 which was collected from coal plants and coal ash dumps under the Coal Ash Rule [@EIP2020]

### Variables {#variables}
<!-- TALK ABOUT VARIABLES AND WELLS/SITES HERE AND WHAT THEY MEAN -->

* a coal ash site consists of multiple disposal areas

* within these disposal areas lie multiple wells

* each observation represents a well

* wells are split into 2 different types - upgradient and downgradient wells

* variables consist of information regard chemical contaminant concentrations and specifics regarding the well

* from the 19 different contaminants (antimony, arsenic, boron, etc. ...) a major problem is that some wells only have measurements for certain chemicals and don't have them for others

* we are currently using information from plants within illinois but there is data for all the states in the US


<!--chapter:end:01-introduction.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Methodology {#methodology}

## Plan of Action {#planofaction}

* we wanted to identify these contaminated upgradient wells and then "correct" these measurements

* we will use manual code to flag contaminated vs noncontaminated wells (filter) using threshold values (this table is from coal ash pdf, but could make manual one)

<!-- insert picture here -->

```{r, echo = FALSE, abridged_table1, fig.cap="Table of Threshold Values", out.width = '100%'}
knitr::include_graphics(path = "figures/abridged_table1.png")
```

* firstly, we used agglomerative hierarchical clustering to identify contaminated upgradient wells in our 'illinois' dataset (thoughts, maybe we want to expand/use a bigger dataset) using Ward's Method

* then, we separated our data into two parts -- one dataset containing these contaminated upgradient wells and another dataset containing UNcontaminated upgradient wells
 
* then, we randomly sampled (with replacement) (500) times from the measurements of the chemical from non-contaminated upgradient wells to create an empirical distribution of naturally occurring chemical levels. this serves as the set of imputed "corrected" measurements of the chemical for each contaminated upgradient well

* then, we identify the specific 'disposal_area' that the contaminated wells belong to and FILTERED to have a dataset contain only the downgradient wells that corresponded to the upgradient wells -- calculating the average of the downgradient wells (for the illinois dataset, we only had contaminated upgradient wells from TWO disposal areas)

* finally, we subtracted each of the (500) imputed upgradient measurements from the average downgradient measure. This creates a distribution of (500) values of the contaminant concentrations caused by the disposal area. 

* we can then take the median of these (500) values as the estimate of the contamination caused by the disposal area (for the given chemical) and then use the 2.5 percentile and 97.5 percentile of the distribution as a bootstrap-type confidence interval.

* we found that the first disposal area didn't have any obvious contamination b/c the difference that we calculated (upgrad - downgradient) was mostly 0, while for the second disposal area the different was much greater than 0

<!-- need to talk about the methods that were used like clustering, bootstrapping, etc.) -->

## Clustering {#clustering}

* unsupervised ml task whose goal is to divide the data in to clusters without knowing what the groups will look like beforehand [@Lantz2013]

* used mainly for knowledge discovery rather than prediction [@Lantz2013]

* many different ways to go about conducting a clustering based investigation, k-means clustering is the method used to try to find relationships between the wells

* our reasons to using this is to see whether if we can identify contaminated wells from uncontaminated wells (we don't anticipate it working due to the messed-up data, but MAYBE we would want to do some sort of study where we 1. run clustering with the messed up data and compare it to 2. run clustering with the corrected data (whatever that might be))

### K-Means Clustering {#kmeans}

* very popular and widely used clustering algorithm even since its inception decades ago [@Lantz2013]

* STRENGTHS: uses simple ideas to identify clsuters that can be explained in non-statistical terms, is flexible and has lots of parameters which can be adjusted to address its issues, and it is efficient [@Lantz2013]

* WEAKNESSES: not as sophisticated than some recent clustering techniques which have arisen recently, since it uses randomness within it, the clusters which it finds is not guaranteed to be optimal, requires a guess as to how many clusters may naturally exist in the data in order for the algorithm to run [@Lantz2013]

* HOW IT WORKS: (add in later, if relevant?)

<!--chapter:end:02-methodology.Rmd-->

# Corrections {-}

A list of corrections after submission to department.

Corrections may be made to the body of the thesis, but every such correction will be acknowledged in a list under the heading “Corrections,” along with the statement “When originally submitted, this honors thesis contained some errors which have been corrected in the current version. Here is a list of the errors that were corrected.” This list will be given on a sheet or sheets to be appended to the thesis. Corrections to spelling, grammar, or typography may be acknowledged by a general statement such as “30 spellings were corrected in various places in the thesis, and the notation for definite integral was changed in approximately 10 places.” However, any correction that affects the meaning of a sentence or paragraph should be described in careful detail. The files samplethesis.tex and samplethesis.pdf show what the "Corrections" section should look like. Questions about what should appear in the “Corrections” should be directed to the Chair.


<!--chapter:end:98-corrections.Rmd-->

<!--
This content below must go last in the thesis document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->

<!--
\backmatter indicates that the next (and final) chapter is the references
-->
\backmatter

<!-- 
You can change the name of the References chapter, but make sure to leave the {-} option to keep the chapter un-numbered.
-->
# References {-}

<!--
\noindent removes the indentation of the first entry.
-->
\noindent

<!--
The \setlength lines below create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->
\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}


<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->
---
nocite: | 
  <!--@Kelderman2019-->
...

<!--chapter:end:99-references.Rmd-->

